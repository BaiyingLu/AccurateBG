{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "from cgms_data_seg_diatrend import CGMSDataSeg\n",
    "from cnn_ohio import regressor, regressor_transfer, test_ckpt\n",
    "from data_reader_DiaTrend import DataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n",
      "List of devices available to TensorFlow:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# New method in TensorFlow 2.x:\n",
    "# This will list the devices TensorFlow recognizes\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"List of devices available to TensorFlow:\")\n",
    "print(tf.config.list_physical_devices())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example subjects\n",
    "file_names = [\n",
    "    'Subject11',\n",
    "    'Subject26', \n",
    "    'Subject3', \n",
    "    'Subject30', \n",
    "    'Subject31', \n",
    "    'Subject36', \n",
    "    'Subject15', \n",
    "    'Subject37', \n",
    "    'Subject38', \n",
    "    'Subject39', \n",
    "    'Subject41', \n",
    "    'Subject42', \n",
    "    'Subject43',\n",
    "    'Subject5', \n",
    "    'Subject6', \n",
    "    'Subject8', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved with 237796 records.\n",
      "Testing data saved with 59450 records.\n",
      "Training data saved with 58584 records.\n",
      "Testing data saved with 14646 records.\n",
      "Training data saved with 318856 records.\n",
      "Testing data saved with 79714 records.\n",
      "Training data saved with 39179 records.\n",
      "Testing data saved with 9795 records.\n",
      "Training data saved with 40054 records.\n",
      "Testing data saved with 10014 records.\n",
      "Training data saved with 21895 records.\n",
      "Testing data saved with 5474 records.\n",
      "Training data saved with 170774 records.\n",
      "Testing data saved with 42694 records.\n",
      "Training data saved with 22718 records.\n",
      "Testing data saved with 5680 records.\n",
      "Training data saved with 22183 records.\n",
      "Testing data saved with 5546 records.\n",
      "Training data saved with 20896 records.\n",
      "Testing data saved with 5224 records.\n",
      "Training data saved with 19178 records.\n",
      "Testing data saved with 4795 records.\n",
      "Training data saved with 17768 records.\n",
      "Testing data saved with 4442 records.\n",
      "Training data saved with 14820 records.\n",
      "Testing data saved with 3706 records.\n",
      "Training data saved with 304163 records.\n",
      "Testing data saved with 76041 records.\n",
      "Training data saved with 335394 records.\n",
      "Testing data saved with 83849 records.\n",
      "Training data saved with 299285 records.\n",
      "Testing data saved with 74822 records.\n"
     ]
    }
   ],
   "source": [
    "for subj in file_names:\n",
    "    subject = pd.read_excel(f\"C:/Users/baiyi/OneDrive/Desktop/BGprediction/DiaTrend/dataset/{subj}.xlsx\",\"CGM\")\n",
    "    split_index = int(len(subject) * 0.8)\n",
    "    # Split the DataFrame\n",
    "    train_df = subject[:split_index]\n",
    "    test_df = subject[split_index:]\n",
    "\n",
    "    # Save the DataFrames to CSV files\n",
    "    train_df.to_csv(f'C:/Users/baiyi/OneDrive/Desktop/BGprediction/DiaTrend/train/{subj}_training_data.csv', index=False)\n",
    "    test_df.to_csv(f'C:/Users/baiyi/OneDrive/Desktop/BGprediction/DiaTrend/test/{subj}_testing_data.csv', index=False)\n",
    "\n",
    "    # Optionally, confirm the operation\n",
    "    print(f\"Training data saved with {len(train_df)} records.\")\n",
    "    print(f\"Testing data saved with {len(test_df)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to do the train_test split\n",
    "# First 80% in training\n",
    "# Last 20% in test\n",
    "# split_index = int(len(subject) * 0.8)\n",
    "# # Split the DataFrame\n",
    "# train_df = subject[:split_index]\n",
    "# test_df = subject[split_index:]\n",
    "\n",
    "# # Save the DataFrames to CSV files\n",
    "# train_df.to_csv(f'C:/Users/baiyi/OneDrive/Desktop/BGprediction/DiaTrend/train/{subj}_training_data.csv', index=False)\n",
    "# test_df.to_csv(f'C:/Users/baiyi/OneDrive/Desktop/BGprediction/DiaTrend/test/{subj}_testing_data.csv', index=False)\n",
    "\n",
    "# # Optionally, confirm the operation\n",
    "# print(f\"Training data saved with {len(train_df)} records.\")\n",
    "# print(f\"Testing data saved with {len(test_df)} records.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named df\n",
    "subject['date'] = pd.to_datetime(subject['date'])  # Convert 'date' column to datetime if not already\n",
    "subject.sort_values('date', inplace=True)  # Sort the DataFrame by the 'date' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming self.interval_timedelta is set, for example:\n",
    "interval_timedelta = datetime.timedelta(minutes=6)  # Example timedelta of 6 minutes, providing a range for latency\n",
    "\n",
    "# Create a list to store the results\n",
    "res = []\n",
    "\n",
    "# Initialize the first group\n",
    "if not subject.empty:\n",
    "    current_group = [subject.iloc[0]['mg/dl']]\n",
    "    last_time = subject.iloc[0]['date']\n",
    "\n",
    "# Iterate over rows in DataFrame starting from the second row\n",
    "for index, row in subject.iloc[1:].iterrows():\n",
    "    current_time = row['date']\n",
    "    if (current_time - last_time) <= interval_timedelta:\n",
    "        # If the time difference is within the limit, add to the current group\n",
    "        current_group.append(row['mg/dl'])\n",
    "    else:\n",
    "        # Otherwise, start a new group\n",
    "        res.append(current_group)\n",
    "        current_group = [row['mg/dl']]\n",
    "    last_time = current_time\n",
    "\n",
    "# Add the last group if it's not empty\n",
    "if current_group:\n",
    "    res.append(current_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_DiaTrend(path):\n",
    "\n",
    "    subject = pd.read_csv(path)\n",
    "    subject['date'] = pd.to_datetime(subject['date'], errors='coerce')  # Convert 'date' column to datetime if not already\n",
    "    print(subject['date'][0])\n",
    "    subject.sort_values('date', inplace=True)  # Sort the DataFrame by the 'date' column\n",
    "\n",
    "    # Assuming self.interval_timedelta is set, for example:\n",
    "    interval_timedelta = datetime.timedelta(minutes=6)  # Example timedelta of 6 minutes, providing a range for latency\n",
    "\n",
    "    # Create a list to store the results\n",
    "    res = []\n",
    "\n",
    "    # Initialize the first group\n",
    "    if not subject.empty:\n",
    "        current_group = [subject.iloc[0]['mg/dl']]\n",
    "        last_time = subject.iloc[0]['date']\n",
    "\n",
    "    # Iterate over rows in DataFrame starting from the second row\n",
    "    for index, row in subject.iloc[1:].iterrows():\n",
    "        current_time = row['date']\n",
    "        if (current_time - last_time) <= interval_timedelta:\n",
    "            # If the time difference is within the limit, add to the current group\n",
    "            current_group.append(row['mg/dl'])\n",
    "        else:\n",
    "            # Otherwise, start a new group\n",
    "            res.append(current_group)\n",
    "            current_group = [row['mg/dl']]\n",
    "        last_time = current_time\n",
    "\n",
    "    # Add the last group if it's not empty\n",
    "    if current_group:\n",
    "        res.append(current_group)\n",
    "    \n",
    "    # Filter out groups with fewer than 10 glucose readings\n",
    "    # res = [group for group in res if len(group) >= 10]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For loop to generate res for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subject11_training_data', 'Subject15_training_data', 'Subject26_training_data', 'Subject30_training_data', 'Subject31_training_data', 'Subject36_training_data', 'Subject37_training_data', 'Subject38_training_data', 'Subject39_training_data', 'Subject3_training_data', 'Subject41_training_data', 'Subject42_training_data', 'Subject43_training_data', 'Subject5_training_data', 'Subject6_training_data', 'Subject8_training_data']\n"
     ]
    }
   ],
   "source": [
    "# Fomulate a loop to create a list to include all the files in train and test datset and generate the res for each of them seperately\n",
    "\n",
    "# Define the directory path\n",
    "train_directory_path = r'C:\\Users\\baiyi\\OneDrive\\Desktop\\BGprediction\\DiaTrend\\train'  # Use a raw string for paths on Windows\n",
    "\n",
    "# List files without their extensions\n",
    "train_file_names = [os.path.splitext(file)[0] for file in os.listdir(train_directory_path)\n",
    "              if os.path.isfile(os.path.join(train_directory_path, file))]\n",
    "\n",
    "# Print the list of file names\n",
    "print(train_file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subject11', 'Subject15', 'Subject26', 'Subject30', 'Subject31', 'Subject36', 'Subject37', 'Subject38', 'Subject39', 'Subject3', 'Subject41', 'Subject42', 'Subject43', 'Subject5', 'Subject6', 'Subject8']\n"
     ]
    }
   ],
   "source": [
    "cleaned_subjects = [s.replace(\"_training_data\", \"\") for s in train_file_names]\n",
    "\n",
    "print(cleaned_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subject11_testing_data', 'Subject15_testing_data', 'Subject26_testing_data', 'Subject30_testing_data', 'Subject31_testing_data', 'Subject36_testing_data', 'Subject37_testing_data', 'Subject38_testing_data', 'Subject39_testing_data', 'Subject3_testing_data', 'Subject41_testing_data', 'Subject42_testing_data', 'Subject43_testing_data', 'Subject5_testing_data', 'Subject6_testing_data', 'Subject8_testing_data']\n"
     ]
    }
   ],
   "source": [
    "# Define the directory path\n",
    "test_directory_path = r'C:\\Users\\baiyi\\OneDrive\\Desktop\\BGprediction\\DiaTrend\\test'  # Use a raw string for paths on Windows\n",
    "\n",
    "# List files without their extensions\n",
    "test_file_names = [os.path.splitext(file)[0] for file in os.listdir(test_directory_path)\n",
    "              if os.path.isfile(os.path.join(test_directory_path, file))]\n",
    "\n",
    "# Print the list of file names\n",
    "print(test_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject11_training_data\n",
      "2018-06-19 16:19:32\n",
      "Subject15_training_data\n",
      "2019-10-01 12:51:22\n",
      "Subject26_training_data\n",
      "2021-02-25 17:28:53\n",
      "Subject30_training_data\n",
      "2019-11-20 18:49:52\n",
      "Subject31_training_data\n",
      "2019-06-25 19:28:12\n",
      "Subject36_training_data\n",
      "2019-06-24 13:01:19\n",
      "Subject37_training_data\n",
      "2019-11-20 19:40:51\n",
      "Subject38_training_data\n",
      "2019-11-22 20:13:43\n",
      "Subject39_training_data\n",
      "2019-06-17 14:59:59\n",
      "Subject3_training_data\n",
      "2016-08-25 00:39:41.582000\n",
      "Subject41_training_data\n",
      "2022-02-15 22:27:15\n",
      "Subject42_training_data\n",
      "2019-06-25 19:12:35\n",
      "Subject43_training_data\n",
      "2022-02-24 22:27:55\n",
      "Subject5_training_data\n",
      "2017-03-16 15:43:40\n",
      "Subject6_training_data\n",
      "2017-11-10 10:33:16\n",
      "Subject8_training_data\n",
      "2018-08-13 11:36:19\n"
     ]
    }
   ],
   "source": [
    "train_data = dict()\n",
    "for subj in train_file_names:\n",
    "    print(subj)\n",
    "    subj_path = f'C:/Users/baiyi/OneDrive/Desktop/BGprediction/DiaTrend/train/{subj}.csv'\n",
    "    reader = preprocess_DiaTrend(subj_path)\n",
    "    train_data[subj] = reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject11_testing_data\n",
      "2021-09-18 12:06:10\n",
      "Subject15_testing_data\n",
      "2021-06-11 06:24:07\n",
      "Subject26_testing_data\n",
      "2021-10-03 03:56:22\n",
      "Subject30_testing_data\n",
      "2019-06-30 12:11:34\n",
      "Subject31_testing_data\n",
      "2019-02-04 17:15:31\n",
      "Subject36_testing_data\n",
      "2019-04-02 00:05:54\n",
      "Subject37_testing_data\n",
      "2019-08-27 02:25:05\n",
      "Subject38_testing_data\n",
      "2019-09-03 11:39:11\n",
      "Subject39_testing_data\n",
      "2019-03-30 03:40:14\n",
      "Subject3_testing_data\n",
      "2020-12-15 09:38:16\n",
      "Subject41_testing_data\n",
      "2022-04-25 04:11:13\n",
      "Subject42_testing_data\n",
      "2019-04-20 18:37:49\n",
      "Subject43_testing_data\n",
      "2022-04-20 09:10:25\n",
      "Subject5_testing_data\n",
      "2021-03-30 17:20:32\n",
      "Subject6_testing_data\n",
      "2021-08-15 19:33:17\n",
      "Subject8_testing_data\n",
      "2021-09-07 06:48:39\n"
     ]
    }
   ],
   "source": [
    "# Have not been run\n",
    "test_data = dict()\n",
    "for subj in test_file_names:\n",
    "    print(subj)\n",
    "    subj_path = f'C:/Users/baiyi/OneDrive/Desktop/BGprediction/DiaTrend/test/{subj}.csv'\n",
    "    reader = preprocess_DiaTrend(subj_path)\n",
    "    test_data[subj] = reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "ph = 6\n",
    "path = \"../diatrend_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-19 16:19:32\n",
      "Reading 571 segments\n"
     ]
    }
   ],
   "source": [
    "# a dumb dataset instance\n",
    "train_dataset = CGMSDataSeg(\n",
    "    \"ohio\", \"C:/Users/baiyi/OneDrive/Desktop/BGprediction/DiaTrend/train/Subject11_training_data.csv\", 5\n",
    ")\n",
    "sampling_horizon = 12\n",
    "prediction_horizon = ph\n",
    "scale = 0.01\n",
    "outtype = \"Same\"\n",
    "# train on training dataset\n",
    "# k_size, nblock, nn_size, nn_layer, learning_rate, batch_size, epoch, beta\n",
    "with open(f'../diatrend_results/config.json') as json_file:\n",
    "    config = json.load(json_file)\n",
    "argv = (\n",
    "    config[\"k_size\"],\n",
    "    config[\"nblock\"],\n",
    "    config[\"nn_size\"],\n",
    "    config[\"nn_layer\"],\n",
    "    config[\"learning_rate\"],\n",
    "    config[\"batch_size\"],\n",
    "    epoch,\n",
    "    config[\"beta\"],\n",
    ")\n",
    "l_type = config[\"loss\"]\n",
    "# test on patients data\n",
    "outdir = os.path.join(path, f\"ph_{prediction_horizon}_{l_type}\")\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "all_errs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subject11',\n",
       " 'Subject15',\n",
       " 'Subject26',\n",
       " 'Subject3',\n",
       " 'Subject30',\n",
       " 'Subject31',\n",
       " 'Subject36',\n",
       " 'Subject37',\n",
       " 'Subject38',\n",
       " 'Subject39',\n",
       " 'Subject41',\n",
       " 'Subject42',\n",
       " 'Subject43',\n",
       " 'Subject5',\n",
       " 'Subject6',\n",
       " 'Subject8']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Subject11_training_data', 'Subject15_training_data', 'Subject26_training_data', 'Subject30_training_data', 'Subject31_training_data', 'Subject36_training_data', 'Subject37_training_data', 'Subject38_training_data', 'Subject39_training_data', 'Subject3_training_data', 'Subject41_training_data', 'Subject42_training_data', 'Subject43_training_data', 'Subject5_training_data', 'Subject6_training_data', 'Subject8_training_data'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Subject15',\n",
       " 'Subject26',\n",
       " 'Subject3',\n",
       " 'Subject30',\n",
       " 'Subject31',\n",
       " 'Subject36',\n",
       " 'Subject37',\n",
       " 'Subject38',\n",
       " 'Subject39',\n",
       " 'Subject41',\n",
       " 'Subject42',\n",
       " 'Subject43',\n",
       " 'Subject5',\n",
       " 'Subject6',\n",
       " 'Subject8'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pids = set(cleaned_subjects) - set([pid])\n",
    "train_pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain data: 320622559.02958834\n",
      "Building dataset, requesting data from 0 to 273724\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 43430/1327730\n",
      "Found 273724 continuous time series\n",
      "Data shape: (1371162, 12), Train/test: 1371160/2\n",
      "Train test ratio: 685580.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  21424\n",
      "Epoch 0, train loss: 0.283173\n",
      "Epoch 1, train loss: 0.217323\n",
      "Epoch 2, train loss: 0.268464\n",
      "Epoch 3, train loss: 0.247767\n",
      "Epoch 4, train loss: 0.236048\n",
      "Epoch 5, train loss: 0.279901\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\training\\saver.py:1064: remove_checkpoint (from tensorflow.python.checkpoint.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Epoch 6, train loss: 0.213192\n",
      "Epoch 7, train loss: 0.216362\n",
      "Epoch 8, train loss: 0.219945\n",
      "Epoch 9, train loss: 0.276606\n",
      "2021-09-18 12:06:10\n",
      "Reading 111 segments\n",
      "Building dataset, requesting data from 0 to 111\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 111 continuous time series\n",
      "Data shape: (58045, 12), Train/test: 1/58044\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2018-06-19 16:19:32\n",
      "Reading 571 segments\n",
      "Building dataset, requesting data from 0 to 571\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 14766/215065\n",
      "Found 571 continuous time series\n",
      "Data shape: (229833, 12), Train/test: 229831/2\n",
      "Train test ratio: 114915.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_diatrend.CGMSDataSeg object at 0x000001539EF259F0>\n",
      "Epoch 0, test loss: 0.327670\n",
      "Epoch 1, test loss: 0.315210\n",
      "Epoch 2, test loss: 0.321230\n",
      "Epoch 3, test loss: 0.316083\n",
      "Epoch 4, test loss: 0.315170\n",
      "Epoch 5, test loss: 0.314665\n",
      "Epoch 6, test loss: 0.318745\n",
      "Epoch 7, test loss: 0.316293\n",
      "Epoch 8, test loss: 0.317249\n",
      "Epoch 9, test loss: 0.317517\n",
      "Pretrain data: 332931933.0295884\n",
      "Building dataset, requesting data from 0 to 274117\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 50706/1382090\n",
      "Found 274117 continuous time series\n",
      "Data shape: (1432798, 12), Train/test: 1432796/2\n",
      "Train test ratio: 716398.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  22387\n",
      "Epoch 0, train loss: 0.287478\n",
      "Epoch 1, train loss: 0.237530\n",
      "Epoch 2, train loss: 0.219357\n",
      "Epoch 3, train loss: 0.254405\n",
      "Epoch 4, train loss: 0.201624\n",
      "Epoch 5, train loss: 0.241307\n",
      "Epoch 6, train loss: 0.223374\n",
      "Epoch 7, train loss: 0.311055\n",
      "Epoch 8, train loss: 0.206935\n",
      "Epoch 9, train loss: 0.252541\n",
      "2021-06-11 06:24:07\n",
      "Reading 49 segments\n",
      "Building dataset, requesting data from 0 to 49\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 49 continuous time series\n",
      "Data shape: (41994, 12), Train/test: 1/41993\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2019-10-01 12:51:22\n",
      "Reading 178 segments\n",
      "Building dataset, requesting data from 0 to 178\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 7490/160705\n",
      "Found 178 continuous time series\n",
      "Data shape: (168197, 12), Train/test: 168195/2\n",
      "Train test ratio: 84097.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_diatrend.CGMSDataSeg object at 0x000001538092FD60>\n",
      "Epoch 0, test loss: 0.216961\n",
      "Epoch 1, test loss: 0.219808\n",
      "Epoch 2, test loss: 0.221293\n",
      "Epoch 3, test loss: 0.216228\n",
      "Epoch 4, test loss: 0.215442\n",
      "Epoch 5, test loss: 0.216487\n",
      "Epoch 6, test loss: 0.223450\n",
      "Epoch 7, test loss: 0.216746\n",
      "Epoch 8, test loss: 0.225325\n",
      "Epoch 9, test loss: 0.216188\n",
      "Pretrain data: 347177618.0295884\n",
      "Building dataset, requesting data from 0 to 274216\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 58054/1485503\n",
      "Found 274216 continuous time series\n",
      "Data shape: (1543559, 12), Train/test: 1543557/2\n",
      "Train test ratio: 771778.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  24118\n",
      "Epoch 0, train loss: 0.227512\n",
      "Epoch 1, train loss: 0.368549\n",
      "Epoch 2, train loss: 0.304828\n",
      "Epoch 3, train loss: 0.297025\n",
      "Epoch 4, train loss: 0.304315\n",
      "Epoch 5, train loss: 0.244067\n",
      "Epoch 6, train loss: 0.233478\n",
      "Epoch 7, train loss: 0.190254\n",
      "Epoch 8, train loss: 0.283329\n",
      "Epoch 9, train loss: 0.241401\n",
      "2021-10-03 03:56:22\n",
      "Reading 31 segments\n",
      "Building dataset, requesting data from 0 to 31\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 31 continuous time series\n",
      "Data shape: (14236, 12), Train/test: 1/14235\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2021-02-25 17:28:53\n",
      "Reading 79 segments\n",
      "Building dataset, requesting data from 0 to 79\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 142/57292\n",
      "Found 79 continuous time series\n",
      "Data shape: (57436, 12), Train/test: 57434/2\n",
      "Train test ratio: 28717.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_diatrend.CGMSDataSeg object at 0x00000153F7F94DF0>\n",
      "Epoch 0, test loss: 0.284007\n",
      "Epoch 1, test loss: 0.275309\n",
      "Epoch 2, test loss: 0.271326\n",
      "Epoch 3, test loss: 0.271639\n",
      "Epoch 4, test loss: 0.283019\n",
      "Epoch 5, test loss: 0.271554\n",
      "Epoch 6, test loss: 0.272951\n",
      "Epoch 7, test loss: 0.292567\n",
      "Epoch 8, test loss: 0.284631\n",
      "Epoch 9, test loss: 0.273493\n",
      "Pretrain data: 303034105.02958834\n",
      "Building dataset, requesting data from 0 to 4098\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 56423/1508974\n",
      "Found 4098 continuous time series\n",
      "Data shape: (1565399, 12), Train/test: 1565397/2\n",
      "Train test ratio: 782698.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  24459\n",
      "Epoch 0, train loss: 0.266382\n",
      "Epoch 1, train loss: 0.227014\n",
      "Epoch 2, train loss: 0.260197\n",
      "Epoch 3, train loss: 0.275766\n",
      "Epoch 4, train loss: 0.257235\n",
      "Epoch 5, train loss: 0.262916\n",
      "Epoch 6, train loss: 0.207161\n",
      "Epoch 7, train loss: 0.270690\n",
      "Epoch 8, train loss: 0.247713\n",
      "Epoch 9, train loss: 0.327039\n",
      "2020-12-15 09:38:16\n",
      "Reading 177 segments\n",
      "Building dataset, requesting data from 0 to 177\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 177 continuous time series\n",
      "Data shape: (77621, 12), Train/test: 1/77620\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2016-08-25 00:39:41.582000\n",
      "Reading 270197 segments\n",
      "Building dataset, requesting data from 0 to 270197\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1773/33821\n",
      "Found 270197 continuous time series\n",
      "Data shape: (35596, 12), Train/test: 35594/2\n",
      "Train test ratio: 17797.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_diatrend.CGMSDataSeg object at 0x0000015445E61990>\n",
      "Epoch 0, test loss: 0.269588\n",
      "Epoch 1, test loss: 0.266687\n",
      "Epoch 2, test loss: 0.267479\n",
      "Epoch 3, test loss: 0.267861\n",
      "Epoch 4, test loss: 0.269126\n",
      "Epoch 5, test loss: 0.275148\n",
      "Epoch 6, test loss: 0.270283\n",
      "Epoch 7, test loss: 0.266507\n",
      "Epoch 8, test loss: 0.270289\n",
      "Epoch 9, test loss: 0.269952\n",
      "Pretrain data: 356437519.68303406\n",
      "Building dataset, requesting data from 0 to 274261\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 56506/1505839\n",
      "Found 274261 continuous time series\n",
      "Data shape: (1562347, 12), Train/test: 1562345/2\n",
      "Train test ratio: 781172.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  24411\n",
      "Epoch 0, train loss: 0.305373\n",
      "Epoch 1, train loss: 0.285715\n",
      "Epoch 2, train loss: 0.248779\n",
      "Epoch 3, train loss: 0.290781\n",
      "Epoch 4, train loss: 0.229933\n",
      "Epoch 5, train loss: 0.243750\n",
      "Epoch 6, train loss: 0.255581\n",
      "Epoch 7, train loss: 0.227066\n",
      "Epoch 8, train loss: 0.263983\n",
      "Epoch 9, train loss: 0.297778\n",
      "2019-06-30 12:11:34\n",
      "Reading 10 segments\n",
      "Building dataset, requesting data from 0 to 10\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 10 continuous time series\n",
      "Data shape: (9647, 12), Train/test: 1/9646\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2019-11-20 18:49:52\n",
      "Reading 34 segments\n",
      "Building dataset, requesting data from 0 to 34\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1690/36956\n",
      "Found 34 continuous time series\n",
      "Data shape: (38648, 12), Train/test: 38646/2\n",
      "Train test ratio: 19323.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_diatrend.CGMSDataSeg object at 0x0000015402A8DA50>\n",
      "Epoch 0, test loss: 0.224720\n",
      "Epoch 1, test loss: 0.218755\n",
      "Epoch 2, test loss: 0.209694\n",
      "Epoch 3, test loss: 0.213912\n",
      "Epoch 4, test loss: 0.231391\n",
      "Epoch 5, test loss: 0.219461\n",
      "Epoch 6, test loss: 0.212310\n",
      "Epoch 7, test loss: 0.209146\n",
      "Epoch 8, test loss: 0.209557\n",
      "Epoch 9, test loss: 0.225030\n",
      "Pretrain data: 355912205.6633434\n",
      "Building dataset, requesting data from 0 to 274277\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 57363/1503882\n",
      "Found 274277 continuous time series\n",
      "Data shape: (1561247, 12), Train/test: 1561245/2\n",
      "Train test ratio: 780622.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  24394\n",
      "Epoch 0, train loss: 0.328436\n",
      "Epoch 1, train loss: 0.242185\n",
      "Epoch 2, train loss: 0.339831\n",
      "Epoch 3, train loss: 0.199427\n",
      "Epoch 4, train loss: 0.230072\n",
      "Epoch 5, train loss: 0.242183\n",
      "Epoch 6, train loss: 0.272788\n",
      "Epoch 7, train loss: 0.286353\n",
      "Epoch 8, train loss: 0.309160\n",
      "Epoch 9, train loss: 0.245562\n",
      "2019-02-04 17:15:31\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (9929, 12), Train/test: 1/9928\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2019-06-25 19:28:12\n",
      "Reading 18 segments\n",
      "Building dataset, requesting data from 0 to 18\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 833/38913\n",
      "Found 18 continuous time series\n",
      "Data shape: (39748, 12), Train/test: 39746/2\n",
      "Train test ratio: 19873.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_diatrend.CGMSDataSeg object at 0x0000015445E1BFA0>\n",
      "Epoch 0, test loss: 0.177997\n",
      "Epoch 1, test loss: 0.176165\n",
      "Epoch 2, test loss: 0.177933\n",
      "Epoch 3, test loss: 0.176899\n",
      "Epoch 4, test loss: 0.176367\n",
      "Epoch 5, test loss: 0.176906\n",
      "Epoch 6, test loss: 0.176288\n",
      "Epoch 7, test loss: 0.176003\n",
      "Epoch 8, test loss: 0.177276\n",
      "Epoch 9, test loss: 0.176534\n",
      "Pretrain data: 358759300.78000844\n",
      "Building dataset, requesting data from 0 to 274260\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 58011/1521643\n",
      "Found 274260 continuous time series\n",
      "Data shape: (1579656, 12), Train/test: 1579654/2\n",
      "Train test ratio: 789827.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  24682\n",
      "Epoch 0, train loss: 0.222166\n",
      "Epoch 1, train loss: 0.279863\n",
      "Epoch 2, train loss: 0.236285\n",
      "Epoch 3, train loss: 0.242076\n",
      "Epoch 4, train loss: 0.241830\n",
      "Epoch 5, train loss: 0.241293\n",
      "Epoch 6, train loss: 0.281125\n",
      "Epoch 7, train loss: 0.239036\n",
      "Epoch 8, train loss: 0.210329\n",
      "Epoch 9, train loss: 0.273562\n",
      "2019-04-02 00:05:54\n",
      "Reading 15 segments\n",
      "Building dataset, requesting data from 0 to 15\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 15 continuous time series\n",
      "Data shape: (5219, 12), Train/test: 1/5218\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2019-06-24 13:01:19\n",
      "Reading 35 segments\n",
      "Building dataset, requesting data from 0 to 35\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 185/21152\n",
      "Found 35 continuous time series\n",
      "Data shape: (21339, 12), Train/test: 21337/2\n",
      "Train test ratio: 10668.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_diatrend.CGMSDataSeg object at 0x000001539282B070>\n",
      "Epoch 0, test loss: 0.232575\n",
      "Epoch 1, test loss: 0.233724\n",
      "Epoch 2, test loss: 0.232957\n",
      "Epoch 3, test loss: 0.232522\n",
      "Epoch 4, test loss: 0.232660\n",
      "Epoch 5, test loss: 0.233394\n",
      "Epoch 6, test loss: 0.239506\n",
      "Epoch 7, test loss: 0.232683\n",
      "Epoch 8, test loss: 0.233247\n",
      "Epoch 9, test loss: 0.234717\n",
      "Pretrain data: 358767228.8959991\n",
      "Building dataset, requesting data from 0 to 274268\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 57812/1520844\n",
      "Found 274268 continuous time series\n",
      "Data shape: (1578658, 12), Train/test: 1578656/2\n",
      "Train test ratio: 789328.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  24666\n",
      "Epoch 0, train loss: 0.319881\n",
      "Epoch 1, train loss: 0.277149\n",
      "Epoch 2, train loss: 0.253380\n",
      "Epoch 3, train loss: 0.257864\n",
      "Epoch 4, train loss: 0.186898\n",
      "Epoch 5, train loss: 0.284968\n",
      "Epoch 6, train loss: 0.220532\n",
      "Epoch 7, train loss: 0.244778\n",
      "Epoch 8, train loss: 0.251484\n",
      "Epoch 9, train loss: 0.246814\n",
      "2019-08-27 02:25:05\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (5595, 12), Train/test: 1/5594\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2019-11-20 19:40:51\n",
      "Reading 27 segments\n",
      "Building dataset, requesting data from 0 to 27\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 384/21951\n",
      "Found 27 continuous time series\n",
      "Data shape: (22337, 12), Train/test: 22335/2\n",
      "Train test ratio: 11167.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_diatrend.CGMSDataSeg object at 0x0000015413AFE860>\n",
      "Epoch 0, test loss: 0.242266\n",
      "Epoch 1, test loss: 0.242783\n",
      "Epoch 2, test loss: 0.242252\n",
      "Epoch 3, test loss: 0.242838\n",
      "Epoch 4, test loss: 0.242896\n",
      "Epoch 5, test loss: 0.243426\n",
      "Epoch 6, test loss: 0.241398\n",
      "Epoch 7, test loss: 0.243007\n",
      "Epoch 8, test loss: 0.243743\n",
      "Epoch 9, test loss: 0.246561\n",
      "Pretrain data: 359472021.4952284\n",
      "Building dataset, requesting data from 0 to 274257\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 57808/1521648\n",
      "Found 274257 continuous time series\n",
      "Data shape: (1579458, 12), Train/test: 1579456/2\n",
      "Train test ratio: 789728.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  24679\n",
      "Epoch 0, train loss: 0.234567\n",
      "Epoch 1, train loss: 0.247594\n",
      "Epoch 2, train loss: 0.243101\n",
      "Epoch 3, train loss: 0.228793\n",
      "Epoch 4, train loss: 0.244752\n",
      "Epoch 5, train loss: 0.235185\n",
      "Epoch 6, train loss: 0.340581\n",
      "Epoch 7, train loss: 0.261273\n",
      "Epoch 8, train loss: 0.222039\n",
      "Epoch 9, train loss: 0.236120\n",
      "2019-09-03 11:39:11\n",
      "Reading 7 segments\n",
      "Building dataset, requesting data from 0 to 7\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 7 continuous time series\n",
      "Data shape: (5427, 12), Train/test: 1/5426\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2019-11-22 20:13:43\n",
      "Reading 38 segments\n",
      "Building dataset, requesting data from 0 to 38\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 388/21147\n",
      "Found 38 continuous time series\n",
      "Data shape: (21537, 12), Train/test: 21535/2\n",
      "Train test ratio: 10767.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_diatrend.CGMSDataSeg object at 0x000001538A6E15A0>\n",
      "Epoch 0, test loss: 0.224486\n",
      "Epoch 1, test loss: 0.224834\n",
      "Epoch 2, test loss: 0.224295\n",
      "Epoch 3, test loss: 0.224401\n",
      "Epoch 4, test loss: 0.223980\n",
      "Epoch 5, test loss: 0.226106\n",
      "Epoch 6, test loss: 0.226918\n",
      "Epoch 7, test loss: 0.224068\n",
      "Epoch 8, test loss: 0.224126\n",
      "Epoch 9, test loss: 0.224700\n",
      "Pretrain data: 359823065.46562856\n",
      "Building dataset, requesting data from 0 to 274203\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 57488/1524130\n",
      "Found 274203 continuous time series\n",
      "Data shape: (1581620, 12), Train/test: 1581618/2\n",
      "Train test ratio: 790809.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  24712\n",
      "Epoch 0, train loss: 0.251063\n",
      "Epoch 1, train loss: 0.258811\n",
      "Epoch 2, train loss: 0.247791\n",
      "Epoch 3, train loss: 0.212490\n",
      "Epoch 4, train loss: 0.210044\n",
      "Epoch 5, train loss: 0.215904\n",
      "Epoch 6, train loss: 0.189272\n",
      "Epoch 7, train loss: 0.267877\n",
      "Epoch 8, train loss: 0.429277\n",
      "Epoch 9, train loss: 0.236908\n",
      "2019-03-30 03:40:14\n",
      "Reading 16 segments\n",
      "Building dataset, requesting data from 0 to 16\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 16 continuous time series\n",
      "Data shape: (4952, 12), Train/test: 1/4951\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2019-06-17 14:59:59\n",
      "Reading 92 segments\n",
      "Building dataset, requesting data from 0 to 92\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 708/18665\n",
      "Found 92 continuous time series\n",
      "Data shape: (19375, 12), Train/test: 19373/2\n",
      "Train test ratio: 9686.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_diatrend.CGMSDataSeg object at 0x00000153805CB580>\n",
      "Epoch 0, test loss: 0.186750\n",
      "Epoch 1, test loss: 0.186276\n",
      "Epoch 2, test loss: 0.186633\n",
      "Epoch 3, test loss: 0.187081\n",
      "Epoch 4, test loss: 0.186738\n",
      "Epoch 5, test loss: 0.187056\n",
      "Epoch 6, test loss: 0.186132\n",
      "Epoch 7, test loss: 0.186768\n",
      "Epoch 8, test loss: 0.186498\n",
      "Epoch 9, test loss: 0.187025\n",
      "Pretrain data: 359731266.02958834\n",
      "Building dataset, requesting data from 0 to 274250\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 57926/1524523\n",
      "Found 274250 continuous time series\n",
      "Data shape: (1582451, 12), Train/test: 1582449/2\n",
      "Train test ratio: 791224.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  24725\n",
      "Epoch 0, train loss: 0.272504\n",
      "Epoch 1, train loss: 0.238184\n",
      "Epoch 2, train loss: 0.260253\n",
      "Epoch 3, train loss: 0.226115\n",
      "Epoch 4, train loss: 0.282741\n",
      "Epoch 5, train loss: 0.265228\n",
      "Epoch 6, train loss: 0.349299\n",
      "Epoch 7, train loss: 0.217720\n",
      "Epoch 8, train loss: 0.251749\n",
      "Epoch 9, train loss: 0.307974\n",
      "2022-04-25 04:11:13\n",
      "Reading 9 segments\n",
      "Building dataset, requesting data from 0 to 9\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 9 continuous time series\n",
      "Data shape: (4683, 12), Train/test: 1/4682\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2022-02-15 22:27:15\n",
      "Reading 45 segments\n",
      "Building dataset, requesting data from 0 to 45\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 270/18272\n",
      "Found 45 continuous time series\n",
      "Data shape: (18544, 12), Train/test: 18542/2\n",
      "Train test ratio: 9271.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_diatrend.CGMSDataSeg object at 0x000001538473CA30>\n",
      "Epoch 0, test loss: 0.198716\n",
      "Epoch 1, test loss: 0.198924\n",
      "Epoch 2, test loss: 0.196375\n",
      "Epoch 3, test loss: 0.196391\n",
      "Epoch 4, test loss: 0.197201\n",
      "Epoch 5, test loss: 0.196876\n",
      "Epoch 6, test loss: 0.196648\n",
      "Epoch 7, test loss: 0.197080\n",
      "Epoch 8, test loss: 0.196780\n",
      "Epoch 9, test loss: 0.196300\n",
      "Pretrain data: 360213592.1942884\n",
      "Building dataset, requesting data from 0 to 274246\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 57362/1526681\n",
      "Found 274246 continuous time series\n",
      "Data shape: (1584045, 12), Train/test: 1584043/2\n",
      "Train test ratio: 792021.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  24750\n",
      "Epoch 0, train loss: 0.267442\n",
      "Epoch 1, train loss: 0.211216\n",
      "Epoch 2, train loss: 0.231834\n",
      "Epoch 3, train loss: 0.211655\n",
      "Epoch 4, train loss: 0.252122\n",
      "Epoch 5, train loss: 0.247224\n",
      "Epoch 6, train loss: 0.259348\n",
      "Epoch 7, train loss: 0.224871\n",
      "Epoch 8, train loss: 0.201997\n",
      "Epoch 9, train loss: 0.253828\n",
      "2019-04-20 18:37:49\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 12 continuous time series\n",
      "Data shape: (4251, 12), Train/test: 1/4250\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2019-06-25 19:12:35\n",
      "Reading 49 segments\n",
      "Building dataset, requesting data from 0 to 49\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 834/16114\n",
      "Found 49 continuous time series\n",
      "Data shape: (16950, 12), Train/test: 16948/2\n",
      "Train test ratio: 8474.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_diatrend.CGMSDataSeg object at 0x0000015411309150>\n",
      "Epoch 0, test loss: 0.156401\n",
      "Epoch 1, test loss: 0.156193\n",
      "Epoch 2, test loss: 0.156428\n",
      "Epoch 3, test loss: 0.163135\n",
      "Epoch 4, test loss: 0.154517\n",
      "Epoch 5, test loss: 0.154531\n",
      "Epoch 6, test loss: 0.158527\n",
      "Epoch 7, test loss: 0.155911\n",
      "Epoch 8, test loss: 0.154441\n",
      "Epoch 9, test loss: 0.158169\n",
      "Pretrain data: 360449306.02958834\n",
      "Building dataset, requesting data from 0 to 274253\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 55967/1530667\n",
      "Found 274253 continuous time series\n",
      "Data shape: (1586636, 12), Train/test: 1586634/2\n",
      "Train test ratio: 793317.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  24791\n",
      "Epoch 0, train loss: 0.223630\n",
      "Epoch 1, train loss: 0.256775\n",
      "Epoch 2, train loss: 0.239254\n",
      "Epoch 3, train loss: 0.193221\n",
      "Epoch 4, train loss: 0.270045\n",
      "Epoch 5, train loss: 0.203292\n",
      "Epoch 6, train loss: 0.348207\n",
      "Epoch 7, train loss: 0.262296\n",
      "Epoch 8, train loss: 0.180046\n",
      "Epoch 9, train loss: 0.227522\n",
      "2022-04-20 09:10:25\n",
      "Reading 2 segments\n",
      "Building dataset, requesting data from 0 to 2\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 2 continuous time series\n",
      "Data shape: (3672, 12), Train/test: 1/3671\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2022-02-24 22:27:55\n",
      "Reading 42 segments\n",
      "Building dataset, requesting data from 0 to 42\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 2229/12128\n",
      "Found 42 continuous time series\n",
      "Data shape: (14359, 12), Train/test: 14357/2\n",
      "Train test ratio: 7178.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_diatrend.CGMSDataSeg object at 0x000001538A736140>\n",
      "Epoch 0, test loss: 0.268497\n",
      "Epoch 1, test loss: 0.257092\n",
      "Epoch 2, test loss: 0.262686\n",
      "Epoch 3, test loss: 0.257323\n",
      "Epoch 4, test loss: 0.261381\n",
      "Epoch 5, test loss: 0.258877\n",
      "Epoch 6, test loss: 0.258081\n",
      "Epoch 7, test loss: 0.258045\n",
      "Epoch 8, test loss: 0.257385\n",
      "Epoch 9, test loss: 0.259674\n",
      "Pretrain data: 307732768.02958834\n",
      "Building dataset, requesting data from 0 to 272924\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 47586/1269943\n",
      "Found 272924 continuous time series\n",
      "Data shape: (1317531, 12), Train/test: 1317529/2\n",
      "Train test ratio: 658764.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  20586\n",
      "Epoch 0, train loss: 0.223043\n",
      "Epoch 1, train loss: 0.265477\n",
      "Epoch 2, train loss: 0.251871\n",
      "Epoch 3, train loss: 0.290679\n",
      "Epoch 4, train loss: 0.186288\n",
      "Epoch 5, train loss: 0.272998\n",
      "Epoch 6, train loss: 0.299102\n",
      "Epoch 7, train loss: 0.248772\n",
      "Epoch 8, train loss: 0.236144\n",
      "Epoch 9, train loss: 0.273425\n",
      "2021-03-30 17:20:32\n",
      "Reading 124 segments\n",
      "Building dataset, requesting data from 0 to 124\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 124 continuous time series\n",
      "Data shape: (74312, 12), Train/test: 1/74311\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2017-03-16 15:43:40\n",
      "Reading 1371 segments\n",
      "Building dataset, requesting data from 0 to 1371\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 10610/272852\n",
      "Found 1371 continuous time series\n",
      "Data shape: (283464, 12), Train/test: 283462/2\n",
      "Train test ratio: 141731.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_diatrend.CGMSDataSeg object at 0x0000015445E125C0>\n",
      "Epoch 0, test loss: 0.201733\n",
      "Epoch 1, test loss: 0.201873\n",
      "Epoch 2, test loss: 0.201910\n",
      "Epoch 3, test loss: 0.202146\n",
      "Epoch 4, test loss: 0.210953\n",
      "Epoch 5, test loss: 0.203182\n",
      "Epoch 6, test loss: 0.201534\n",
      "Epoch 7, test loss: 0.201475\n",
      "Epoch 8, test loss: 0.201045\n",
      "Epoch 9, test loss: 0.200687\n",
      "Pretrain data: 304170282.02958834\n",
      "Building dataset, requesting data from 0 to 273319\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 52121/1227810\n",
      "Found 273319 continuous time series\n",
      "Data shape: (1279933, 12), Train/test: 1279931/2\n",
      "Train test ratio: 639965.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  19998\n",
      "Epoch 0, train loss: 0.235631\n",
      "Epoch 1, train loss: 0.215317\n",
      "Epoch 2, train loss: 0.230483\n",
      "Epoch 3, train loss: 0.289325\n",
      "Epoch 4, train loss: 0.196826\n",
      "Epoch 5, train loss: 0.201383\n",
      "Epoch 6, train loss: 0.243981\n",
      "Epoch 7, train loss: 0.208127\n",
      "Epoch 8, train loss: 0.216388\n",
      "Epoch 9, train loss: 0.290988\n",
      "2021-08-15 19:33:17\n",
      "Reading 179 segments\n",
      "Building dataset, requesting data from 0 to 179\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 179 continuous time series\n",
      "Data shape: (81369, 12), Train/test: 1/81368\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2017-11-10 10:33:16\n",
      "Reading 976 segments\n",
      "Building dataset, requesting data from 0 to 976\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6075/314985\n",
      "Found 976 continuous time series\n",
      "Data shape: (321062, 12), Train/test: 321060/2\n",
      "Train test ratio: 160530.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_diatrend.CGMSDataSeg object at 0x000001538A72B490>\n",
      "Epoch 0, test loss: 0.243315\n",
      "Epoch 1, test loss: 0.244346\n",
      "Epoch 2, test loss: 0.243903\n",
      "Epoch 3, test loss: 0.246831\n",
      "Epoch 4, test loss: 0.244564\n",
      "Epoch 5, test loss: 0.243429\n",
      "Epoch 6, test loss: 0.256386\n",
      "Epoch 7, test loss: 0.244416\n",
      "Epoch 8, test loss: 0.243890\n",
      "Epoch 9, test loss: 0.243638\n",
      "Pretrain data: 297874299.02958834\n",
      "Building dataset, requesting data from 0 to 273752\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 48377/1260046\n",
      "Found 273752 continuous time series\n",
      "Data shape: (1308425, 12), Train/test: 1308423/2\n",
      "Train test ratio: 654211.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 12), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 12), dtype=float32)\n",
      "line73: Shape of y: (None, 12)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  20444\n",
      "Epoch 0, train loss: 0.286890\n",
      "Epoch 1, train loss: 0.234486\n",
      "Epoch 2, train loss: 0.227509\n",
      "Epoch 3, train loss: 0.207330\n",
      "Epoch 4, train loss: 0.274256\n",
      "Epoch 5, train loss: 0.238616\n",
      "Epoch 6, train loss: 0.204871\n",
      "Epoch 7, train loss: 0.272958\n",
      "Epoch 8, train loss: 0.177049\n",
      "Epoch 9, train loss: 0.252147\n",
      "2021-09-07 06:48:39\n",
      "Reading 122 segments\n",
      "Building dataset, requesting data from 0 to 122\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1/0\n",
      "Found 122 continuous time series\n",
      "Data shape: (73236, 12), Train/test: 1/73235\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2018-08-13 11:36:19\n",
      "Reading 543 segments\n",
      "Building dataset, requesting data from 0 to 543\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 9819/282749\n",
      "Found 543 continuous time series\n",
      "Data shape: (292570, 12), Train/test: 292568/2\n",
      "Train test ratio: 146284.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../diatrend_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_diatrend.CGMSDataSeg object at 0x0000015388C8E080>\n",
      "Epoch 0, test loss: 0.307957\n",
      "Epoch 1, test loss: 0.300161\n",
      "Epoch 2, test loss: 0.300885\n",
      "Epoch 3, test loss: 0.300793\n",
      "Epoch 4, test loss: 0.301242\n",
      "Epoch 5, test loss: 0.299681\n",
      "Epoch 6, test loss: 0.300087\n",
      "Epoch 7, test loss: 0.300281\n",
      "Epoch 8, test loss: 0.307821\n",
      "Epoch 9, test loss: 0.304763\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Mismatch between array dtype ('<U32') and format specifier ('%s %.4f %.4f')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\baiyi\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\numpy\\lib\\npyio.py:1623\u001b[0m, in \u001b[0;36msavetxt\u001b[1;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1623\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m newline\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not numpy.str_",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 82\u001b[0m\n\u001b[0;32m     80\u001b[0m     all_errs\u001b[38;5;241m.\u001b[39mappend([pid] \u001b[38;5;241m+\u001b[39m errs)\n\u001b[0;32m     81\u001b[0m all_errs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_errs)\n\u001b[1;32m---> 82\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavetxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/errors.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_errs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m%.4f\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m%.4f\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# label pair:(groundTruth, y_pred)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\baiyi\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\numpy\\lib\\npyio.py:1625\u001b[0m, in \u001b[0;36msavetxt\u001b[1;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[0;32m   1623\u001b[0m             v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtuple\u001b[39m(row) \u001b[38;5;241m+\u001b[39m newline\n\u001b[0;32m   1624\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1625\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch between array dtype (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1626\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat specifier (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1627\u001b[0m                             \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mstr\u001b[39m(X\u001b[38;5;241m.\u001b[39mdtype), \u001b[38;5;28mformat\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m         fh\u001b[38;5;241m.\u001b[39mwrite(v)\n\u001b[0;32m   1630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(footer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Mismatch between array dtype ('<U32') and format specifier ('%s %.4f %.4f')"
     ]
    }
   ],
   "source": [
    "# Loop\n",
    "cleaned_subjects.sort()\n",
    "standard = False  # do not use standard\n",
    "all_errs = []\n",
    "for pid in cleaned_subjects: # First 9 as subset, can be an example\n",
    "    train_pids = set(cleaned_subjects) - set([pid])\n",
    "    local_train_data = []\n",
    "    for k in train_pids:\n",
    "        local_train_data += train_data[k + \"_training_data\"]\n",
    "    print(f\"Pretrain data: {sum([sum(x) for x in local_train_data])}\")\n",
    "    \n",
    "    train_dataset.data = local_train_data\n",
    "    train_dataset.set_cutpoint = -1\n",
    "    train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    regressor(train_dataset, *argv, l_type, outdir)\n",
    "    # Fine-tune and test\n",
    "    # target_test_dataset = CGMSDataSeg(\n",
    "    #     \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\", 6\n",
    "    # )\n",
    "    target_test_dataset = CGMSDataSeg(\n",
    "    \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/BGprediction/DiaTrend/test/{pid}_testing_data.csv\", 5\n",
    "    )\n",
    "    target_test_dataset.set_cutpoint = 1\n",
    "    target_test_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        0.01,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    # target_train_dataset = CGMSDataSeg(\n",
    "    #     \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\", 5\n",
    "    # )\n",
    "    target_train_dataset = CGMSDataSeg(\n",
    "    \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/BGprediction/DiaTrend/train/{pid}_training_data.csv\", 5\n",
    "    )\n",
    "    target_train_dataset.set_cutpoint = -1\n",
    "    target_train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    err, labels = test_ckpt(target_test_dataset, outdir)\n",
    "    errs = [err]\n",
    "    transfer_res = [labels]\n",
    "    for i in range(1, 2):\n",
    "        err, labels = regressor_transfer(\n",
    "            target_train_dataset,\n",
    "            target_test_dataset,\n",
    "            config[\"batch_size\"],\n",
    "            epoch,\n",
    "            outdir,\n",
    "            i,\n",
    "        )\n",
    "        errs.append(err)\n",
    "        transfer_res.append(labels)\n",
    "    transfer_res = np.concatenate(transfer_res, axis=1)\n",
    "    np.savetxt(\n",
    "        f\"{outdir}/{pid}.txt\",\n",
    "        transfer_res,\n",
    "        fmt=\"%.4f %.4f %.4f %.4f\",\n",
    "    )\n",
    "    all_errs.append([pid] + errs)\n",
    "all_errs = np.array(all_errs)\n",
    "np.savetxt(f\"{outdir}/errors.txt\", all_errs, fmt=\"%s %.4f %.4f\")\n",
    "# label pair:(groundTruth, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Subject11', '0.32737887', '0.31751657'],\n",
       "       ['Subject15', '0.21702431', '0.21618843'],\n",
       "       ['Subject26', '0.31711185', '0.27349338'],\n",
       "       ['Subject3', '0.2685392', '0.2699521'],\n",
       "       ['Subject30', '0.21345535', '0.22502957'],\n",
       "       ['Subject31', '0.1765919', '0.17653388'],\n",
       "       ['Subject36', '0.23704633', '0.23471707'],\n",
       "       ['Subject37', '0.24713682', '0.24656081'],\n",
       "       ['Subject38', '0.23195074', '0.2247002'],\n",
       "       ['Subject39', '0.1977754', '0.18702544'],\n",
       "       ['Subject41', '0.19897726', '0.19630045'],\n",
       "       ['Subject42', '0.16103005', '0.15816924'],\n",
       "       ['Subject43', '0.26570606', '0.25967422'],\n",
       "       ['Subject5', '0.20555699', '0.2006869'],\n",
       "       ['Subject6', '0.2507571', '0.24363846'],\n",
       "       ['Subject8', '0.31099993', '0.30476284']], dtype='<U32')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['Subject11', '0.32737887', '0.31751657'],\n",
    "['Subject15', '0.21702431', '0.21618843'],\n",
    "['Subject26', '0.31711185', '0.27349338'],\n",
    "['Subject3', '0.2685392', '0.2699521'],\n",
    "['Subject30', '0.21345535', '0.22502957'],\n",
    "['Subject31', '0.1765919', '0.17653388'],\n",
    "['Subject36', '0.23704633', '0.23471707'],\n",
    "['Subject37', '0.24713682', '0.24656081'],\n",
    "['Subject38', '0.23195074', '0.2247002'],\n",
    "['Subject39', '0.1977754', '0.18702544'],\n",
    "['Subject41', '0.19897726', '0.19630045'],\n",
    "['Subject42', '0.16103005', '0.15816924'],\n",
    "['Subject43', '0.26570606', '0.25967422'],\n",
    "['Subject5', '0.20555699', '0.2006869'],\n",
    "['Subject6', '0.2507571', '0.24363846'],\n",
    "['Subject8', '0.31099993', '0.30476284']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['Subject11', '0.3257632', '0.31918257'],\n",
    "['Subject15', '0.21759439', '0.21790388'],\n",
    "['Subject26', '0.28789735', '0.27718654'],\n",
    "['Subject3', '0.26801455', '0.26734337'],\n",
    "['Subject30', '0.21501394', '0.20972419'],\n",
    "['Subject31', '0.1775641', '0.17692266'],\n",
    "['Subject36', '0.23820741', '0.23973823'],\n",
    "['Subject37', '0.24739642', '0.24186915'],\n",
    "['Subject38', '0.2375784', '0.22447872'],\n",
    "['Subject39', '0.2006417', '0.18739489'],\n",
    "['Subject41', '0.19930626', '0.1959955'],\n",
    "['Subject42', '0.16283263', '0.15715173'],\n",
    "['Subject43', '0.26754174', '0.26674312'],\n",
    "['Subject5', '0.20940122', '0.20045878'],\n",
    "['Subject6', '0.25433964', '0.24411151'],\n",
    "['Subject8', '0.32000047', '0.30507126']，## 30 sampling horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the second column: 0.23918988500000002\n",
      "Average of the third column: 0.23343434749999997\n"
     ]
    }
   ],
   "source": [
    "# Convert the second and third columns to floats\n",
    "second_column = all_errs[:, 1].astype(float)\n",
    "third_column = all_errs[:, 2].astype(float)\n",
    "\n",
    "# Calculate the average\n",
    "average_second_column = np.mean(second_column)\n",
    "average_third_column = np.mean(third_column)\n",
    "\n",
    "print(\"Average of the second column:\", average_second_column)\n",
    "print(\"Average of the third column:\", average_third_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the second column: 0.23931833875\n",
      "Average of the third column: 0.23320475625\n"
     ]
    }
   ],
   "source": [
    "# Convert the second and third columns to floats\n",
    "second_column = all_errs[:, 1].astype(float)\n",
    "third_column = all_errs[:, 2].astype(float)\n",
    "\n",
    "# Calculate the average\n",
    "average_second_column = np.mean(second_column)\n",
    "average_third_column = np.mean(third_column)\n",
    "\n",
    "print(\"Average of the second column:\", average_second_column)\n",
    "print(\"Average of the third column:\", average_third_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, if the previous cell run into an issue but all result txt files are ready\n",
    "# You can run this to evaluate:\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# List all files and directories in the current directory\n",
    "files_and_directories = os.listdir('.')\n",
    "\n",
    "# Filter for files that end with .txt\n",
    "txt_files = [file for file in files_and_directories if file.endswith('.txt')]\n",
    "\n",
    "\n",
    "# Read the data from the text file\n",
    "def calcuate_rmse(file):\n",
    "    data = np.loadtxt(file)  # Make sure to replace 'data.txt' with your actual file path\n",
    "    print(file)\n",
    "    # Splitting the data into groundtruth and predictions\n",
    "    groundtruth = data[:, 0]  # First column as ground truth (also same as third column)\n",
    "    predictions_1 = data[:, 1]  # Second column as predictions from method 1\n",
    "    predictions_2 = data[:, 3]  # Fourth column as predictions from method 2\n",
    "\n",
    "    # Function to calculate RMSE\n",
    "    def calculate_rmse(true_values, predictions):\n",
    "        mse = np.mean((true_values - predictions) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return rmse\n",
    "\n",
    "    # Calculate RMSE for each method\n",
    "    rmse_method_1 = calculate_rmse(groundtruth, predictions_1)\n",
    "    rmse_method_2 = calculate_rmse(groundtruth, predictions_2)\n",
    "\n",
    "    print(\"RMSE for Method 1:\", rmse_method_1)\n",
    "    print(\"RMSE for Method 2:\", rmse_method_2)\n",
    "    return rmse_method_1\n",
    "\n",
    "\n",
    "rmse_list = []\n",
    "for f in txt_files[1:]:\n",
    "    rmse1 = calcuate_rmse(f)\n",
    "    print(rmse1)\n",
    "    rmse_list.append(rmse1)\n",
    "\n",
    "print(np.average(rmse_list))\n",
    "\n",
    "print(rmse_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
