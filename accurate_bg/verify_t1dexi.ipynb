{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "from cgms_data_seg_t1dexi import CGMSDataSeg\n",
    "from cnn_ohio import regressor, regressor_transfer, test_ckpt\n",
    "from data_reader_T1DEXI import DataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n",
      "List of devices available to TensorFlow:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# New method in TensorFlow 2.x:\n",
    "# This will list the devices TensorFlow recognizes\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"List of devices available to TensorFlow:\")\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1000', '1004', '1010', '1012', '1013', '1014', '1015', '1016', '1020', '1021', '1022', '1024', '103', '1033', '1039', '104', '1043', '1046', '1050', '1051', '1067', '107', '1072', '1076', '1077', '1082', '1090', '1093', '11', '110', '1100', '1103', '1107', '1110', '1112', '1115', '1118', '1119', '1120', '1121', '1123', '1127', '1138', '1139', '114', '1141', '1142', '1143', '1146', '1149', '115', '1152', '1155', '1156', '1157', '1158', '1161', '1163', '1170', '1171', '1185', '1193', '1194', '1195', '1199', '1201', '1202', '1203', '1205', '1206', '1210', '1211', '1219', '1221', '1224', '1230', '1239', '1243', '1247', '1248', '1249', '1250', '1253', '1255', '1257', '1261', '1265', '1266', '127', '1271', '1273', '1281', '1283', '1286', '1287', '1291', '1292', '1297', '1302', '1303', '1304', '1305', '1307', '1311', '1312', '1322', '1323', '1325', '1327', '1328', '1329', '1330', '1335', '1336', '1338', '1339', '1343', '1344', '1345', '1348', '1351', '1354', '1357', '1359', '1361', '1362', '1363', '1365', '1369', '137', '1377', '1378', '1381', '1385', '1386', '1387', '1396', '14', '1400', '1406', '1408', '1410', '1412', '1416', '1418', '1422', '1425', '1426', '1427', '143', '1433', '1434', '1435', '1438', '1439', '144', '1441', '1444', '145', '1451', '1453', '1456', '1457', '1459', '1462', '1464', '1476', '1484', '1490', '1493', '1494', '1495', '1497', '1500', '1501', '1503', '1507', '1509', '1517', '152', '1520', '1528', '1535', '1536', '1540', '1542', '1543', '1550', '1552', '1554', '1558', '1559', '1563', '1566', '1567', '1568', '1580', '1586', '159', '1596', '1602', '1603', '1604', '1606', '1611', '1615', '1616', '1617', '1621', '1622', '1625', '163', '1632', '1634', '1635', '1636', '1638', '1644', '1647', '1649', '1650', '1656', '1658', '166', '1660', '1666', '1668', '1669', '167', '1673', '1683', '1689', '1695', '1696', '1698', '1704', '1711', '1712', '1713', '1714', '1717', '1718', '1719', '1722', '1726', '173', '174', '18', '183', '187', '191', '194', '202', '214', '217', '219', '221', '223', '226', '227', '235', '24', '248', '25', '252', '253', '254', '255', '256', '261', '263', '267', '270', '284', '288', '29', '290', '295', '301', '304', '306', '308', '312', '313', '316', '317', '32', '320', '323', '329', '334', '336', '34', '341', '349', '352', '354', '355', '356', '364', '367', '37', '372', '374', '378', '380', '384', '385', '386', '389', '390', '393', '398', '4', '402', '404', '406', '410', '414', '419', '421', '422', '423', '425', '428', '43', '431', '433', '434', '435', '436', '438', '446', '453', '455', '463', '467', '468', '469', '471', '475', '477', '479', '48', '482', '483', '484', '486', '491', '498', '499', '50', '504', '506', '507', '509', '511', '515', '521', '528', '531', '532', '539', '54', '545', '548', '549', '550', '556', '558', '564', '567', '573', '576', '577', '58', '583', '591', '598', '60', '602', '603', '606', '608', '610', '612', '614', '616', '620', '629', '63', '646', '648', '652', '653', '654', '655', '66', '662', '666', '667', '67', '670', '671', '674', '676', '677', '678', '683', '69', '692', '695', '702', '704', '707', '71', '715', '716', '723', '729', '733', '734', '735', '740', '741', '742', '748', '749', '756', '760', '761', '762', '766', '767', '769', '77', '771', '773', '775', '78', '781', '791', '796', '80', '811', '812', '816', '824', '829', '830', '831', '834', '838', '839', '84', '843', '85', '852', '854', '856', '862', '865', '870', '873', '886', '888', '889', '890', '893', '894', '896', '897', '900', '905', '907', '911', '920', '930', '932', '933', '943', '945', '946', '948', '95', '953', '956', '958', '963', '965', '966', '97', '970', '971', '974', '976', '979', '981', '985', '987', '988']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the directory path\n",
    "# directory_path = r'C:\\Users\\baiyi\\OneDrive\\Desktop\\BGprediction\\LB_split'  # Use a raw string for paths on Windows\n",
    "\n",
    "# # List files without their extensions\n",
    "# file_names = [os.path.splitext(file)[0] for file in os.listdir(directory_path)\n",
    "#               if os.path.isfile(os.path.join(directory_path, file))]\n",
    "\n",
    "# # Print the list of file names\n",
    "# print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_preprocess_cgm_data(subj):\n",
    "    subject = pd.read_csv(f'C:/Users/baiyi/OneDrive/Desktop/BGprediction/LB_split/{subj}.csv')\n",
    "    subject_cgm = subject.loc[subject['LBCAT'] == 'CGM']\n",
    "    # Convert the 'LBDTC' column to datetime\n",
    "    subject_cgm['LBDTC'] = pd.to_datetime(subject_cgm['LBDTC'])\n",
    "    # Extract the date part\n",
    "    subject_cgm['date'] = subject_cgm['LBDTC'].dt.date\n",
    "    return subject_cgm\n",
    "    # Count the number of occurrences of each unique date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the continuity of the selected items\n",
    "# def calendar_array(dates, data):\n",
    "#     i, j = zip(*[d.isocalendar()[1:] for d in dates])\n",
    "#     i = np.array(i) - min(i)\n",
    "#     j = np.array(j) - 1\n",
    "#     ni = max(i) + 1\n",
    "\n",
    "#     calendar = np.nan * np.zeros((ni, 7))\n",
    "#     calendar[i, j] = data\n",
    "#     return i, j, calendar\n",
    "\n",
    "\n",
    "# def calendar_heatmap(ax, dates, data):\n",
    "#     i, j, calendar = calendar_array(dates, data)\n",
    "#     im = ax.imshow(calendar, interpolation='none', cmap='summer')\n",
    "#     label_days(ax, dates, i, j, calendar)\n",
    "#     label_months(ax, dates, i, j, calendar)\n",
    "    \n",
    "#     # Adjust the colorbar\n",
    "#     cbar = ax.figure.colorbar(im, ax=ax, fraction=0.02, pad=0.04)\n",
    "#     cbar.ax.tick_params(labelsize=8)  # Adjust colorbar tick label size if needed\n",
    "\n",
    "# def label_days(ax, dates, i, j, calendar):\n",
    "#     ni, nj = calendar.shape\n",
    "#     day_of_month = np.nan * np.zeros((ni, 7))\n",
    "#     day_of_month[i, j] = [d.day for d in dates]\n",
    "\n",
    "#     for (i, j), day in np.ndenumerate(day_of_month):\n",
    "#         if np.isfinite(day):\n",
    "#             ax.text(j, i, int(day), ha='center', va='center')\n",
    "\n",
    "#     ax.set(xticks=np.arange(7), \n",
    "#            xticklabels=['M', 'T', 'W', 'R', 'F', 'S', 'S'])\n",
    "#     ax.xaxis.tick_top()\n",
    "\n",
    "# def label_months(ax, dates, i, j, calendar):\n",
    "#     month_labels = np.array(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul',\n",
    "#                              'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "#     months = np.array([d.month for d in dates])\n",
    "#     uniq_months = sorted(set(months))\n",
    "#     yticks = [i[months == m].mean() for m in uniq_months]\n",
    "#     labels = [month_labels[m - 1] for m in uniq_months]\n",
    "#     ax.set(yticks=yticks)\n",
    "#     ax.set_yticklabels(labels, rotation=90)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime as dt\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# # Look at the contiunity of the selected subjects, no need to run\n",
    "\n",
    "# for d in selected_subjects:\n",
    "    \n",
    "#     subject_cgm = read_preprocess_cgm_data(d)\n",
    "\n",
    "#     date_counts = subject_cgm['date'].value_counts().sort_index()\n",
    "#     dates, data = date_counts.index, date_counts.values\n",
    "#     fig, ax = plt.subplots(figsize=(6, 10))\n",
    "#     calendar_heatmap(ax, dates, data)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = ['854',\n",
    " '979',\n",
    " '816',\n",
    " '953',\n",
    " '981',\n",
    " '1617',\n",
    " '1343',\n",
    " '987',\n",
    " '255',\n",
    " '907',\n",
    " '856',\n",
    " '354',\n",
    " '894',\n",
    " '862',\n",
    " '900',\n",
    " '695']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved with 7870 records.\n",
      "Training data saved with 7930 records.\n",
      "Training data saved with 7606 records.\n",
      "Training data saved with 7632 records.\n",
      "Training data saved with 2880 records.\n",
      "Training data saved with 7972 records.\n",
      "Training data saved with 7603 records.\n",
      "Training data saved with 7372 records.\n",
      "Training data saved with 7503 records.\n",
      "Training data saved with 7475 records.\n",
      "Training data saved with 7945 records.\n",
      "Training data saved with 8011 records.\n",
      "Training data saved with 7855 records.\n",
      "Training data saved with 7537 records.\n",
      "Training data saved with 5611 records.\n",
      "Training data saved with 7963 records.\n",
      "Training data saved with 7965 records.\n",
      "Training data saved with 7969 records.\n"
     ]
    }
   ],
   "source": [
    "# For population\n",
    "# Extract those samples from dataset\n",
    "\n",
    "# for subj in overlap:\n",
    "#     subject = pd.read_csv(f'C:/Users/baiyi/OneDrive/Desktop/BGprediction/LB_split/{subj}.csv')\n",
    "#     subject_cgm = subject.loc[subject['LBCAT'] == 'CGM']\n",
    "    \n",
    "#     # split_index = int(len(subject_cgm) * 0.8)\n",
    "#     # Split the DataFrame\n",
    "#     train_df = subject\n",
    "\n",
    "\n",
    "#     # Save the DataFrames to CSV files\n",
    "#     train_df.to_csv(f'C:/Users/baiyi/OneDrive/Desktop/BGprediction/LB_split/population_training/{subj}_training_data.csv', index=False)\n",
    "#     # test_df.to_csv(f'C:/Users/baiyi/OneDrive/Desktop/BGprediction/LB_split/testing/{subj}_testing_data.csv', index=False)\n",
    "\n",
    "#     # Optionally, confirm the operation\n",
    "#     print(f\"Training data saved with {len(train_df)} records.\")\n",
    "#     # print(f\"Testing data saved with {len(test_df)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved with 6295 records.\n",
      "Testing data saved with 1575 records.\n",
      "Training data saved with 6343 records.\n",
      "Testing data saved with 1587 records.\n",
      "Training data saved with 6084 records.\n",
      "Testing data saved with 1522 records.\n",
      "Training data saved with 6104 records.\n",
      "Testing data saved with 1528 records.\n",
      "Training data saved with 2303 records.\n",
      "Testing data saved with 577 records.\n",
      "Training data saved with 6376 records.\n",
      "Testing data saved with 1596 records.\n",
      "Training data saved with 6081 records.\n",
      "Testing data saved with 1522 records.\n",
      "Training data saved with 5896 records.\n",
      "Testing data saved with 1476 records.\n",
      "Training data saved with 6001 records.\n",
      "Testing data saved with 1502 records.\n",
      "Training data saved with 5979 records.\n",
      "Testing data saved with 1496 records.\n",
      "Training data saved with 6355 records.\n",
      "Testing data saved with 1590 records.\n",
      "Training data saved with 6408 records.\n",
      "Testing data saved with 1603 records.\n",
      "Training data saved with 6283 records.\n",
      "Testing data saved with 1572 records.\n",
      "Training data saved with 6028 records.\n",
      "Testing data saved with 1509 records.\n",
      "Training data saved with 4488 records.\n",
      "Testing data saved with 1123 records.\n",
      "Training data saved with 6369 records.\n",
      "Testing data saved with 1594 records.\n",
      "Training data saved with 6371 records.\n",
      "Testing data saved with 1594 records.\n",
      "Training data saved with 6374 records.\n",
      "Testing data saved with 1595 records.\n"
     ]
    }
   ],
   "source": [
    "# Divide training and testing\n",
    "# Generate folder for training and test data\n",
    "\n",
    "for subj in overlap:\n",
    "    subject = pd.read_csv(f'C:/Users/baiyi/OneDrive/Desktop/BGprediction/LB_split/{subj}.csv')\n",
    "    subject_cgm = subject.loc[subject['LBCAT'] == 'CGM']\n",
    "    \n",
    "    split_index = int(len(subject_cgm) * 0.8)\n",
    "    # Split the DataFrame\n",
    "    train_df = subject[:split_index]\n",
    "    test_df = subject[split_index:]\n",
    "\n",
    "    # Save the DataFrames to CSV files\n",
    "    train_df.to_csv(f'C:/Users/baiyi/OneDrive/Desktop/BGprediction/LB_split/training/{subj}_training_data.csv', index=False)\n",
    "    test_df.to_csv(f'C:/Users/baiyi/OneDrive/Desktop/BGprediction/LB_split/testing/{subj}_testing_data.csv', index=False)\n",
    "\n",
    "    # Optionally, confirm the operation\n",
    "    print(f\"Training data saved with {len(train_df)} records.\")\n",
    "    print(f\"Testing data saved with {len(test_df)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_preprocess_cgm_training(subj):\n",
    "    subject_cgm = pd.read_csv(f'C:/Users/baiyi/OneDrive/Desktop/BGprediction/LB_split/training/{subj}_training_data.csv')\n",
    "\n",
    "    subject_cgm['LBDTC'] = pd.to_datetime(subject_cgm['LBDTC'])\n",
    "    # Extract the date part\n",
    "    subject_cgm['date'] = subject_cgm['LBDTC'].dt.date\n",
    "    subject_cgm_concise = subject_cgm[[\"LBORRES\", \"LBDTC\"]]\n",
    "    return subject_cgm_concise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example \n",
    "subject_cgm = read_preprocess_cgm_training(overlap[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LBORRES</th>\n",
       "      <th>LBDTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71.0</td>\n",
       "      <td>2021-03-12 00:02:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.0</td>\n",
       "      <td>2021-03-12 00:07:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.0</td>\n",
       "      <td>2021-03-12 00:12:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.0</td>\n",
       "      <td>2021-03-12 00:17:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.0</td>\n",
       "      <td>2021-03-12 00:22:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6338</th>\n",
       "      <td>83.0</td>\n",
       "      <td>2021-04-03 08:48:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6339</th>\n",
       "      <td>83.0</td>\n",
       "      <td>2021-04-03 08:53:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6340</th>\n",
       "      <td>83.0</td>\n",
       "      <td>2021-04-03 08:58:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6341</th>\n",
       "      <td>84.0</td>\n",
       "      <td>2021-04-03 09:03:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6342</th>\n",
       "      <td>84.0</td>\n",
       "      <td>2021-04-03 09:08:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6343 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LBORRES               LBDTC\n",
       "0        71.0 2021-03-12 00:02:14\n",
       "1        69.0 2021-03-12 00:07:13\n",
       "2        67.0 2021-03-12 00:12:13\n",
       "3        66.0 2021-03-12 00:17:13\n",
       "4        65.0 2021-03-12 00:22:13\n",
       "...       ...                 ...\n",
       "6338     83.0 2021-04-03 08:48:24\n",
       "6339     83.0 2021-04-03 08:53:24\n",
       "6340     83.0 2021-04-03 08:58:24\n",
       "6341     84.0 2021-04-03 09:03:24\n",
       "6342     84.0 2021-04-03 09:08:25\n",
       "\n",
       "[6343 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_cgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Segment the data into several pieces\n",
    "# Assuming self.interval_timedelta is set, for example:\n",
    "interval_timedelta = datetime.timedelta(minutes=6)  # Example timedelta of 6 minutes, providing a range for latency\n",
    "\n",
    "# Create a list to store the results\n",
    "res = []\n",
    "\n",
    "# Initialize the first group\n",
    "if not subject_cgm.empty:\n",
    "    current_group = [subject_cgm.iloc[0]['LBORRES']]\n",
    "    last_time = subject_cgm.iloc[0]['LBDTC']\n",
    "\n",
    "# Iterate over rows in DataFrame starting from the second row\n",
    "for index, row in subject_cgm.iloc[1:].iterrows():\n",
    "    current_time = row['LBDTC']\n",
    "    if (current_time - last_time) <= interval_timedelta:\n",
    "        # If the time difference is within the limit, add to the current group\n",
    "        current_group.append(row['LBORRES'])\n",
    "    else:\n",
    "        # Otherwise, start a new group\n",
    "        res.append(current_group)\n",
    "        current_group = [row['LBORRES']]\n",
    "    last_time = current_time\n",
    "\n",
    "# Add the last group if it's not empty\n",
    "if current_group:\n",
    "    res.append(current_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_preprocess_cgm_training(subj):\n",
    "    subject_cgm = pd.read_csv(subj)\n",
    "\n",
    "    subject_cgm['LBDTC'] = pd.to_datetime(subject_cgm['LBDTC'])\n",
    "    # Extract the date part\n",
    "    subject_cgm['date'] = subject_cgm['LBDTC'].dt.date\n",
    "    subject_cgm_concise = subject_cgm[[\"LBORRES\", \"LBDTC\"]]\n",
    "    return subject_cgm_concise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preporcess_T1DEXI(subject_id):\n",
    "    subject_cgm = read_preprocess_cgm_training(subject_id)\n",
    "    interval_timedelta = datetime.timedelta(minutes=6)  # Example timedelta of 6 minutes, providing a range for latency\n",
    "\n",
    "    # Create a list to store the results\n",
    "    res = []\n",
    "\n",
    "    # Initialize the first group\n",
    "    if not subject_cgm.empty:\n",
    "        current_group = [subject_cgm.iloc[0]['LBORRES']]\n",
    "        last_time = subject_cgm.iloc[0]['LBDTC']\n",
    "\n",
    "    # Iterate over rows in DataFrame starting from the second row\n",
    "    for index, row in subject_cgm.iloc[1:].iterrows():\n",
    "        current_time = row['LBDTC']\n",
    "        if (current_time - last_time) <= interval_timedelta:\n",
    "            # If the time difference is within the limit, add to the current group\n",
    "            current_group.append(row['LBORRES'])\n",
    "        else:\n",
    "            # Otherwise, start a new group\n",
    "            res.append(current_group)\n",
    "            current_group = [row['LBORRES']]\n",
    "        last_time = current_time\n",
    "\n",
    "    # Add the last group if it's not empty\n",
    "    if current_group:\n",
    "        res.append(current_group)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For loop to generate res for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1343_training_data', '1617_training_data', '255_training_data', '354_training_data', '695_training_data', '816_training_data', '854_training_data', '856_training_data', '862_training_data', '894_training_data', '900_training_data', '907_training_data', '953_training_data', '979_training_data', '981_training_data', '987_training_data']\n"
     ]
    }
   ],
   "source": [
    "# Define the directory path\n",
    "train_directory_path = r'C:\\Users\\baiyi\\OneDrive\\Desktop\\BGprediction\\LB_split\\training'  # Use a raw string for paths on Windows\n",
    "\n",
    "# List files without their extensions\n",
    "train_file_names = [os.path.splitext(file)[0] for file in os.listdir(train_directory_path)\n",
    "              if os.path.isfile(os.path.join(train_directory_path, file))]\n",
    "\n",
    "# Print the list of file names\n",
    "print(train_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1343_testing_data', '1617_testing_data', '255_testing_data', '354_testing_data', '695_testing_data', '816_testing_data', '854_testing_data', '856_testing_data', '862_testing_data', '894_testing_data', '900_testing_data', '907_testing_data', '953_testing_data', '979_testing_data', '981_testing_data', '987_testing_data']\n"
     ]
    }
   ],
   "source": [
    "# Define the directory path\n",
    "test_directory_path = r'C:\\Users\\baiyi\\OneDrive\\Desktop\\BGprediction\\LB_split\\testing'  # Use a raw string for paths on Windows\n",
    "\n",
    "# List files without their extensions\n",
    "test_file_names = [os.path.splitext(file)[0] for file in os.listdir(test_directory_path)\n",
    "              if os.path.isfile(os.path.join(test_directory_path, file))]\n",
    "\n",
    "# Print the list of file names\n",
    "print(test_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1343_training_data\n",
      "1617_training_data\n",
      "255_training_data\n",
      "354_training_data\n",
      "695_training_data\n",
      "816_training_data\n",
      "854_training_data\n",
      "856_training_data\n",
      "862_training_data\n",
      "894_training_data\n",
      "900_training_data\n",
      "907_training_data\n",
      "953_training_data\n",
      "979_training_data\n",
      "981_training_data\n",
      "987_training_data\n"
     ]
    }
   ],
   "source": [
    "train_data = dict()\n",
    "for subj in train_file_names:\n",
    "    print(subj)\n",
    "    subj_path = f'C:/Users/baiyi/OneDrive/Desktop/BGprediction/LB_split/training/{subj}.csv'\n",
    "    reader = preporcess_T1DEXI(subj_path)\n",
    "    train_data[subj] = reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1343_testing_data\n",
      "1617_testing_data\n",
      "255_testing_data\n",
      "354_testing_data\n",
      "695_testing_data\n",
      "816_testing_data\n",
      "854_testing_data\n",
      "856_testing_data\n",
      "862_testing_data\n",
      "894_testing_data\n",
      "900_testing_data\n",
      "907_testing_data\n",
      "953_testing_data\n",
      "979_testing_data\n",
      "981_testing_data\n",
      "987_testing_data\n"
     ]
    }
   ],
   "source": [
    "# Have not been run\n",
    "test_data = dict()\n",
    "for subj in test_file_names:\n",
    "    print(subj)\n",
    "    subj_path = f'C:/Users/baiyi/OneDrive/Desktop/BGprediction/LB_split/testing/{subj}.csv'\n",
    "    reader = preporcess_T1DEXI(subj_path)\n",
    "    test_data[subj] = reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 20\n",
    "\n",
    "ph = 6 # Prediction horizon\n",
    "path = \"../t1dexi_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_subjects = [s.replace(\"_training_data\", \"\") for s in train_file_names]\n",
    "cleaned_subjects.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-01 00:01:51\n",
      "Reading 8 segments\n"
     ]
    }
   ],
   "source": [
    "# a dumb dataset instance\n",
    "train_dataset = CGMSDataSeg(\n",
    "    \"ohio\", \"C:/Users/baiyi/OneDrive/Desktop/BGprediction/LB_split/training/255_training_data.csv\", 5\n",
    ")\n",
    "sampling_horizon = 7\n",
    "prediction_horizon = ph\n",
    "scale = 0.01\n",
    "outtype = \"Same\"\n",
    "# train on training dataset\n",
    "# k_size, nblock, nn_size, nn_layer, learning_rate, batch_size, epoch, beta\n",
    "with open(f'../t1dexi_results/config.json') as json_file:\n",
    "    config = json.load(json_file)\n",
    "argv = (\n",
    "    config[\"k_size\"],\n",
    "    config[\"nblock\"],\n",
    "    config[\"nn_size\"],\n",
    "    config[\"nn_layer\"],\n",
    "    config[\"learning_rate\"],\n",
    "    config[\"batch_size\"],\n",
    "    epoch,\n",
    "    config[\"beta\"],\n",
    ")\n",
    "l_type = config[\"loss\"]\n",
    "# test on patients data\n",
    "outdir = os.path.join(path, f\"ph_{prediction_horizon}_{l_type}\")\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "all_errs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain data: 12359130.0\n",
      "Building dataset, requesting data from 0 to 294\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6458/80116\n",
      "Found 294 continuous time series\n",
      "Data shape: (86576, 7), Train/test: 86574/2\n",
      "Train test ratio: 43287.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 7), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 7), dtype=float32)\n",
      "line73: Shape of y: (None, 7)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1352\n",
      "Epoch 0, train loss: 0.275283\n",
      "Epoch 1, train loss: 0.199644\n",
      "Epoch 2, train loss: 0.212992\n",
      "Epoch 3, train loss: 0.174614\n",
      "Epoch 4, train loss: 0.182365\n",
      "Epoch 5, train loss: 0.206565\n",
      "WARNING:tensorflow:From C:\\Users\\baiyi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\training\\saver.py:1064: remove_checkpoint (from tensorflow.python.checkpoint.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Epoch 6, train loss: 0.236761\n",
      "Epoch 7, train loss: 0.142241\n",
      "Epoch 8, train loss: 0.112913\n",
      "Epoch 9, train loss: 0.194917\n",
      "Epoch 10, train loss: 0.162354\n",
      "Epoch 11, train loss: 0.174137\n",
      "Epoch 12, train loss: 0.169851\n",
      "Epoch 13, train loss: 0.145419\n",
      "Epoch 14, train loss: 0.159308\n",
      "Epoch 15, train loss: 0.182463\n",
      "Epoch 16, train loss: 0.180535\n",
      "Epoch 17, train loss: 0.173560\n",
      "Epoch 18, train loss: 0.157529\n",
      "Epoch 19, train loss: 0.168314\n",
      "2020-10-21 12:53:50\n",
      "Reading 2 segments\n",
      "Building dataset, requesting data from 0 to 2\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1/0\n",
      "Found 2 continuous time series\n",
      "Data shape: (1562, 7), Train/test: 1/1561\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2020-09-29 00:04:38\n",
      "Reading 6 segments\n",
      "Building dataset, requesting data from 0 to 6\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 347/5874\n",
      "Found 6 continuous time series\n",
      "Data shape: (6223, 7), Train/test: 6221/2\n",
      "Train test ratio: 3110.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_t1dexi.CGMSDataSeg object at 0x0000021FCB597DC0>\n",
      "Epoch 0, test loss: 0.141509\n",
      "Epoch 1, test loss: 0.138905\n",
      "Epoch 2, test loss: 0.138749\n",
      "Epoch 3, test loss: 0.137666\n",
      "Epoch 4, test loss: 0.137384\n",
      "Epoch 5, test loss: 0.139973\n",
      "Epoch 6, test loss: 0.142363\n",
      "Epoch 7, test loss: 0.136336\n",
      "Epoch 8, test loss: 0.138682\n",
      "Epoch 9, test loss: 0.136645\n",
      "Epoch 10, test loss: 0.137615\n",
      "Epoch 11, test loss: 0.136509\n",
      "Epoch 12, test loss: 0.147266\n",
      "Epoch 13, test loss: 0.138225\n",
      "Epoch 14, test loss: 0.139569\n",
      "Epoch 15, test loss: 0.136396\n",
      "Epoch 16, test loss: 0.136939\n",
      "Epoch 17, test loss: 0.138747\n",
      "Epoch 18, test loss: 0.136104\n",
      "Epoch 19, test loss: 0.140329\n",
      "[[0.7        0.79773438 0.7        0.75521135]\n",
      " [0.73       0.7982893  0.73       0.77025509]\n",
      " [0.72       0.81374931 0.72       0.81468284]\n",
      " ...\n",
      " [2.01       2.08071828 2.01       2.08360624]\n",
      " [1.97       2.05844355 1.97       2.0858283 ]\n",
      " [1.94       2.02698112 1.94       2.07282066]]\n",
      "Pretrain data: 12521155.0\n",
      "Building dataset, requesting data from 0 to 292\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 5799/80751\n",
      "Found 292 continuous time series\n",
      "Data shape: (86552, 7), Train/test: 86550/2\n",
      "Train test ratio: 43275.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 7), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 7), dtype=float32)\n",
      "line73: Shape of y: (None, 7)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1352\n",
      "Epoch 0, train loss: 0.217123\n",
      "Epoch 1, train loss: 0.213330\n",
      "Epoch 2, train loss: 0.202045\n",
      "Epoch 3, train loss: 0.215716\n",
      "Epoch 4, train loss: 0.199141\n",
      "Epoch 5, train loss: 0.204059\n",
      "Epoch 6, train loss: 0.177791\n",
      "Epoch 7, train loss: 0.170082\n",
      "Epoch 8, train loss: 0.154141\n",
      "Epoch 9, train loss: 0.168199\n",
      "Epoch 10, train loss: 0.223063\n",
      "Epoch 11, train loss: 0.122744\n",
      "Epoch 12, train loss: 0.200655\n",
      "Epoch 13, train loss: 0.156252\n",
      "Epoch 14, train loss: 0.136159\n",
      "Epoch 15, train loss: 0.161574\n",
      "Epoch 16, train loss: 0.235129\n",
      "Epoch 17, train loss: 0.134798\n",
      "Epoch 18, train loss: 0.185965\n",
      "Epoch 19, train loss: 0.242118\n",
      "2021-04-03 09:13:24\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1/0\n",
      "Found 3 continuous time series\n",
      "Data shape: (1562, 7), Train/test: 1/1561\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2021-03-12 00:02:14\n",
      "Reading 8 segments\n",
      "Building dataset, requesting data from 0 to 8\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1006/5239\n",
      "Found 8 continuous time series\n",
      "Data shape: (6247, 7), Train/test: 6245/2\n",
      "Train test ratio: 3122.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_t1dexi.CGMSDataSeg object at 0x0000021FCBDEFFD0>\n",
      "Epoch 0, test loss: 0.110814\n",
      "Epoch 1, test loss: 0.116605\n",
      "Epoch 2, test loss: 0.105615\n",
      "Epoch 3, test loss: 0.106181\n",
      "Epoch 4, test loss: 0.107848\n",
      "Epoch 5, test loss: 0.107332\n",
      "Epoch 6, test loss: 0.106967\n",
      "Epoch 7, test loss: 0.105613\n",
      "Epoch 8, test loss: 0.105038\n",
      "Epoch 9, test loss: 0.105076\n",
      "Epoch 10, test loss: 0.106889\n",
      "Epoch 11, test loss: 0.105064\n",
      "Epoch 12, test loss: 0.105125\n",
      "Epoch 13, test loss: 0.105582\n",
      "Epoch 14, test loss: 0.104977\n",
      "Epoch 15, test loss: 0.107589\n",
      "Epoch 16, test loss: 0.104902\n",
      "Epoch 17, test loss: 0.105332\n",
      "Epoch 18, test loss: 0.104840\n",
      "Epoch 19, test loss: 0.106848\n",
      "[[0.8        0.87132877 0.8        0.84088302]\n",
      " [0.84       0.8920759  0.84       0.85923278]\n",
      " [0.89       0.8425411  0.89       0.79583317]\n",
      " ...\n",
      " [1.18       1.22440612 1.18       1.16587377]\n",
      " [1.16       1.19507647 1.16       1.14931667]\n",
      " [1.15       1.16605735 1.15       1.12176621]]\n",
      "Pretrain data: 12485945.0\n",
      "Building dataset, requesting data from 0 to 288\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6066/80773\n",
      "Found 288 continuous time series\n",
      "Data shape: (86841, 7), Train/test: 86839/2\n",
      "Train test ratio: 43419.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 7), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 7), dtype=float32)\n",
      "line73: Shape of y: (None, 7)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1356\n",
      "Epoch 0, train loss: 0.233857\n",
      "Epoch 1, train loss: 0.136877\n",
      "Epoch 2, train loss: 0.176156\n",
      "Epoch 3, train loss: 0.174312\n",
      "Epoch 4, train loss: 0.170800\n",
      "Epoch 5, train loss: 0.136430\n",
      "Epoch 6, train loss: 0.189083\n",
      "Epoch 7, train loss: 0.183905\n",
      "Epoch 8, train loss: 0.201246\n",
      "Epoch 9, train loss: 0.189829\n",
      "Epoch 10, train loss: 0.197233\n",
      "Epoch 11, train loss: 0.163984\n",
      "Epoch 12, train loss: 0.145631\n",
      "Epoch 13, train loss: 0.165244\n",
      "Epoch 14, train loss: 0.168989\n",
      "Epoch 15, train loss: 0.182948\n",
      "Epoch 16, train loss: 0.188600\n",
      "Epoch 17, train loss: 0.181721\n",
      "Epoch 18, train loss: 0.190174\n",
      "Epoch 19, train loss: 0.164128\n",
      "2020-10-12 14:55:22\n",
      "Reading 2 segments\n",
      "Building dataset, requesting data from 0 to 2\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 2 continuous time series\n",
      "Data shape: (1509, 7), Train/test: 1/1508\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2020-09-21 00:04:12\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 739/5217\n",
      "Found 12 continuous time series\n",
      "Data shape: (5958, 7), Train/test: 5956/2\n",
      "Train test ratio: 2978.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_t1dexi.CGMSDataSeg object at 0x0000021FCCA0DB70>\n",
      "Epoch 0, test loss: 0.162032\n",
      "Epoch 1, test loss: 0.160755\n",
      "Epoch 2, test loss: 0.159764\n",
      "Epoch 3, test loss: 0.159497\n",
      "Epoch 4, test loss: 0.160878\n",
      "Epoch 5, test loss: 0.161719\n",
      "Epoch 6, test loss: 0.160025\n",
      "Epoch 7, test loss: 0.159519\n",
      "Epoch 8, test loss: 0.160078\n",
      "Epoch 9, test loss: 0.162644\n",
      "Epoch 10, test loss: 0.157911\n",
      "Epoch 11, test loss: 0.157866\n",
      "Epoch 12, test loss: 0.156943\n",
      "Epoch 13, test loss: 0.158510\n",
      "Epoch 14, test loss: 0.159094\n",
      "Epoch 15, test loss: 0.159476\n",
      "Epoch 16, test loss: 0.157132\n",
      "Epoch 17, test loss: 0.157808\n",
      "Epoch 18, test loss: 0.156797\n",
      "Epoch 19, test loss: 0.156372\n",
      "[[1.3        1.2974596  1.3        1.23016596]\n",
      " [1.25       1.31073093 1.25       1.28777301]\n",
      " [1.19       1.32663822 1.19       1.31474459]\n",
      " ...\n",
      " [0.88       1.02300501 0.88       0.94748431]\n",
      " [0.85       0.98957819 0.85       0.96193051]\n",
      " [0.81       0.94276738 0.81       0.94220096]]\n",
      "Pretrain data: 12307270.0\n",
      "Building dataset, requesting data from 0 to 288\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6658/80155\n",
      "Found 288 continuous time series\n",
      "Data shape: (86815, 7), Train/test: 86813/2\n",
      "Train test ratio: 43406.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 7), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 7), dtype=float32)\n",
      "line73: Shape of y: (None, 7)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1356\n",
      "Epoch 0, train loss: 0.187071\n",
      "Epoch 1, train loss: 0.220488\n",
      "Epoch 2, train loss: 0.199620\n",
      "Epoch 3, train loss: 0.181651\n",
      "Epoch 4, train loss: 0.144782\n",
      "Epoch 5, train loss: 0.200335\n",
      "Epoch 6, train loss: 0.183064\n",
      "Epoch 7, train loss: 0.189371\n",
      "Epoch 8, train loss: 0.143207\n",
      "Epoch 9, train loss: 0.159164\n",
      "Epoch 10, train loss: 0.170460\n",
      "Epoch 11, train loss: 0.205731\n",
      "Epoch 12, train loss: 0.183794\n",
      "Epoch 13, train loss: 0.199387\n",
      "Epoch 14, train loss: 0.212022\n",
      "Epoch 15, train loss: 0.205287\n",
      "Epoch 16, train loss: 0.170729\n",
      "Epoch 17, train loss: 0.184835\n",
      "Epoch 18, train loss: 0.212542\n",
      "Epoch 19, train loss: 0.153222\n",
      "2020-11-26 14:37:10\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 3 continuous time series\n",
      "Data shape: (1503, 7), Train/test: 1/1502\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2020-11-04 13:07:58\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 147/5835\n",
      "Found 12 continuous time series\n",
      "Data shape: (5984, 7), Train/test: 5982/2\n",
      "Train test ratio: 2991.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_t1dexi.CGMSDataSeg object at 0x0000022049C3F580>\n",
      "Epoch 0, test loss: 0.166805\n",
      "Epoch 1, test loss: 0.166826\n",
      "Epoch 2, test loss: 0.164990\n",
      "Epoch 3, test loss: 0.164444\n",
      "Epoch 4, test loss: 0.163792\n",
      "Epoch 5, test loss: 0.163109\n",
      "Epoch 6, test loss: 0.163913\n",
      "Epoch 7, test loss: 0.162613\n",
      "Epoch 8, test loss: 0.162537\n",
      "Epoch 9, test loss: 0.162857\n",
      "Epoch 10, test loss: 0.160948\n",
      "Epoch 11, test loss: 0.162850\n",
      "Epoch 12, test loss: 0.172034\n",
      "Epoch 13, test loss: 0.160465\n",
      "Epoch 14, test loss: 0.160390\n",
      "Epoch 15, test loss: 0.160995\n",
      "Epoch 16, test loss: 0.165853\n",
      "Epoch 17, test loss: 0.161826\n",
      "Epoch 18, test loss: 0.163704\n",
      "Epoch 19, test loss: 0.166483\n",
      "[[1.3        1.36719668 1.3        1.44210505]\n",
      " [1.22       1.41930091 1.22       1.45917952]\n",
      " [1.19       1.36054873 1.19       1.41083956]\n",
      " ...\n",
      " [2.19       2.16344857 2.19       2.21574521]\n",
      " [2.04       2.07893181 2.04       2.09396768]\n",
      " [1.84       1.97967935 1.84       1.98857653]]\n",
      "Pretrain data: 12843613.0\n",
      "Building dataset, requesting data from 0 to 299\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6560/83946\n",
      "Found 299 continuous time series\n",
      "Data shape: (90508, 7), Train/test: 90506/2\n",
      "Train test ratio: 45253.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 7), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 7), dtype=float32)\n",
      "line73: Shape of y: (None, 7)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1414\n",
      "Epoch 0, train loss: 0.185675\n",
      "Epoch 1, train loss: 0.182338\n",
      "Epoch 2, train loss: 0.191318\n",
      "Epoch 3, train loss: 0.211531\n",
      "Epoch 4, train loss: 0.215945\n",
      "Epoch 5, train loss: 0.163260\n",
      "Epoch 6, train loss: 0.219245\n",
      "Epoch 7, train loss: 0.157408\n",
      "Epoch 8, train loss: 0.192658\n",
      "Epoch 9, train loss: 0.185272\n",
      "Epoch 10, train loss: 0.154824\n",
      "Epoch 11, train loss: 0.176592\n",
      "Epoch 12, train loss: 0.183372\n",
      "Epoch 13, train loss: 0.177513\n",
      "Epoch 14, train loss: 0.233944\n",
      "Epoch 15, train loss: 0.218736\n",
      "Epoch 16, train loss: 0.133524\n",
      "Epoch 17, train loss: 0.163445\n",
      "Epoch 18, train loss: 0.210359\n",
      "Epoch 19, train loss: 0.146405\n",
      "2020-05-19 23:58:01\n",
      "Reading 8 segments\n",
      "Building dataset, requesting data from 0 to 8\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 8 continuous time series\n",
      "Data shape: (546, 7), Train/test: 1/545\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2020-05-12 00:02:06\n",
      "Reading 1 segments\n",
      "Building dataset, requesting data from 0 to 1\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 245/2044\n",
      "Found 1 continuous time series\n",
      "Data shape: (2291, 7), Train/test: 2289/2\n",
      "Train test ratio: 1144.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_t1dexi.CGMSDataSeg object at 0x0000021FCC947AC0>\n",
      "Epoch 0, test loss: 0.178712\n",
      "Epoch 1, test loss: 0.178247\n",
      "Epoch 2, test loss: 0.179475\n",
      "Epoch 3, test loss: 0.179663\n",
      "Epoch 4, test loss: 0.178118\n",
      "Epoch 5, test loss: 0.178110\n",
      "Epoch 6, test loss: 0.178656\n",
      "Epoch 7, test loss: 0.181486\n",
      "Epoch 8, test loss: 0.178423\n",
      "Epoch 9, test loss: 0.178129\n",
      "Epoch 10, test loss: 0.178645\n",
      "Epoch 11, test loss: 0.178279\n",
      "Epoch 12, test loss: 0.179004\n",
      "Epoch 13, test loss: 0.177800\n",
      "Epoch 14, test loss: 0.179581\n",
      "Epoch 15, test loss: 0.182300\n",
      "Epoch 16, test loss: 0.179057\n",
      "Epoch 17, test loss: 0.177432\n",
      "Epoch 18, test loss: 0.177556\n",
      "Epoch 19, test loss: 0.178176\n",
      "[[1.12       1.05772614 1.12       1.09489536]\n",
      " [1.11       1.1367166  1.11       1.17634428]\n",
      " [1.07       1.16742194 1.07       1.21023822]\n",
      " ...\n",
      " [0.39       0.57227349 0.39       0.56907713]\n",
      " [0.39       0.57209438 0.39       0.56920093]\n",
      " [0.39       0.57128614 0.39       0.56929547]]\n",
      "Pretrain data: 12048251.0\n",
      "Building dataset, requesting data from 0 to 296\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6698/79771\n",
      "Found 296 continuous time series\n",
      "Data shape: (86471, 7), Train/test: 86469/2\n",
      "Train test ratio: 43234.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 7), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 7), dtype=float32)\n",
      "line73: Shape of y: (None, 7)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1351\n",
      "Epoch 0, train loss: 0.199316\n",
      "Epoch 1, train loss: 0.183316\n",
      "Epoch 2, train loss: 0.234067\n",
      "Epoch 3, train loss: 0.138854\n",
      "Epoch 4, train loss: 0.185713\n",
      "Epoch 5, train loss: 0.166733\n",
      "Epoch 6, train loss: 0.159054\n",
      "Epoch 7, train loss: 0.166017\n",
      "Epoch 8, train loss: 0.166560\n",
      "Epoch 9, train loss: 0.161690\n",
      "Epoch 10, train loss: 0.145846\n",
      "Epoch 11, train loss: 0.191043\n",
      "Epoch 12, train loss: 0.156815\n",
      "Epoch 13, train loss: 0.245141\n",
      "Epoch 14, train loss: 0.165559\n",
      "Epoch 15, train loss: 0.176717\n",
      "Epoch 16, train loss: 0.165281\n",
      "Epoch 17, train loss: 0.147481\n",
      "Epoch 18, train loss: 0.171599\n",
      "Epoch 19, train loss: 0.221037\n",
      "2020-10-23 09:01:47\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 3 continuous time series\n",
      "Data shape: (1571, 7), Train/test: 1/1570\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2020-10-01 00:04:45\n",
      "Reading 4 segments\n",
      "Building dataset, requesting data from 0 to 4\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 107/6219\n",
      "Found 4 continuous time series\n",
      "Data shape: (6328, 7), Train/test: 6326/2\n",
      "Train test ratio: 3163.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_t1dexi.CGMSDataSeg object at 0x000002204ED6E9B0>\n",
      "Epoch 0, test loss: 0.178509\n",
      "Epoch 1, test loss: 0.178757\n",
      "Epoch 2, test loss: 0.179254\n",
      "Epoch 3, test loss: 0.177938\n",
      "Epoch 4, test loss: 0.180937\n",
      "Epoch 5, test loss: 0.176701\n",
      "Epoch 6, test loss: 0.186814\n",
      "Epoch 7, test loss: 0.183165\n",
      "Epoch 8, test loss: 0.178768\n",
      "Epoch 9, test loss: 0.175305\n",
      "Epoch 10, test loss: 0.175388\n",
      "Epoch 11, test loss: 0.177068\n",
      "Epoch 12, test loss: 0.176593\n",
      "Epoch 13, test loss: 0.174738\n",
      "Epoch 14, test loss: 0.176920\n",
      "Epoch 15, test loss: 0.178769\n",
      "Epoch 16, test loss: 0.173905\n",
      "Epoch 17, test loss: 0.174838\n",
      "Epoch 18, test loss: 0.174763\n",
      "Epoch 19, test loss: 0.174051\n",
      "[[0.88       1.02872658 0.88       1.042431  ]\n",
      " [0.86       0.96025741 0.86       1.00453067]\n",
      " [0.83       0.93855321 0.83       0.98806959]\n",
      " ...\n",
      " [1.55       1.48070025 1.55       1.50181627]\n",
      " [1.55       1.39090085 1.55       1.411695  ]\n",
      " [1.54       1.41212094 1.54       1.45527101]]\n",
      "Pretrain data: 12529578.0\n",
      "Building dataset, requesting data from 0 to 293\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6554/80235\n",
      "Found 293 continuous time series\n",
      "Data shape: (86791, 7), Train/test: 86789/2\n",
      "Train test ratio: 43394.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 7), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 7), dtype=float32)\n",
      "line73: Shape of y: (None, 7)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1356\n",
      "Epoch 0, train loss: 0.205067\n",
      "Epoch 1, train loss: 0.233907\n",
      "Epoch 2, train loss: 0.191067\n",
      "Epoch 3, train loss: 0.169689\n",
      "Epoch 4, train loss: 0.178983\n",
      "Epoch 5, train loss: 0.126268\n",
      "Epoch 6, train loss: 0.172501\n",
      "Epoch 7, train loss: 0.200946\n",
      "Epoch 8, train loss: 0.186929\n",
      "Epoch 9, train loss: 0.158252\n",
      "Epoch 10, train loss: 0.187304\n",
      "Epoch 11, train loss: 0.169982\n",
      "Epoch 12, train loss: 0.206019\n",
      "Epoch 13, train loss: 0.177922\n",
      "Epoch 14, train loss: 0.162158\n",
      "Epoch 15, train loss: 0.156801\n",
      "Epoch 16, train loss: 0.220989\n",
      "Epoch 17, train loss: 0.198062\n",
      "Epoch 18, train loss: 0.238755\n",
      "Epoch 19, train loss: 0.150082\n",
      "2020-08-05 13:49:55\n",
      "Reading 24 segments\n",
      "Building dataset, requesting data from 0 to 24\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 24 continuous time series\n",
      "Data shape: (1339, 7), Train/test: 1/1338\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2020-07-15 00:04:15\n",
      "Reading 7 segments\n",
      "Building dataset, requesting data from 0 to 7\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 251/5755\n",
      "Found 7 continuous time series\n",
      "Data shape: (6008, 7), Train/test: 6006/2\n",
      "Train test ratio: 3003.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_t1dexi.CGMSDataSeg object at 0x0000021FCCF6D690>\n",
      "Epoch 0, test loss: 0.136343\n",
      "Epoch 1, test loss: 0.133316\n",
      "Epoch 2, test loss: 0.132459\n",
      "Epoch 3, test loss: 0.131652\n",
      "Epoch 4, test loss: 0.131428\n",
      "Epoch 5, test loss: 0.131515\n",
      "Epoch 6, test loss: 0.131661\n",
      "Epoch 7, test loss: 0.131370\n",
      "Epoch 8, test loss: 0.130274\n",
      "Epoch 9, test loss: 0.130123\n",
      "Epoch 10, test loss: 0.130153\n",
      "Epoch 11, test loss: 0.130059\n",
      "Epoch 12, test loss: 0.130039\n",
      "Epoch 13, test loss: 0.130839\n",
      "Epoch 14, test loss: 0.129779\n",
      "Epoch 15, test loss: 0.130707\n",
      "Epoch 16, test loss: 0.129861\n",
      "Epoch 17, test loss: 0.130581\n",
      "Epoch 18, test loss: 0.130036\n",
      "Epoch 19, test loss: 0.129667\n",
      "[[1.06       1.12304556 1.06       1.06099296]\n",
      " [1.04       1.06899655 1.04       1.04114246]\n",
      " [1.03       1.11586237 1.03       1.07617235]\n",
      " ...\n",
      " [0.95       0.57274526 0.95       0.6512816 ]\n",
      " [0.97       0.82726902 0.97       0.81447554]\n",
      " [0.87       0.94438148 0.87       0.90622026]]\n",
      "Pretrain data: 12275705.0\n",
      "Building dataset, requesting data from 0 to 260\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6402/80792\n",
      "Found 260 continuous time series\n",
      "Data shape: (87196, 7), Train/test: 87194/2\n",
      "Train test ratio: 43597.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 7), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 7), dtype=float32)\n",
      "line73: Shape of y: (None, 7)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1362\n",
      "Epoch 0, train loss: 0.158893\n",
      "Epoch 1, train loss: 0.189504\n",
      "Epoch 2, train loss: 0.141298\n",
      "Epoch 3, train loss: 0.196102\n",
      "Epoch 4, train loss: 0.172206\n",
      "Epoch 5, train loss: 0.202875\n",
      "Epoch 6, train loss: 0.150506\n",
      "Epoch 7, train loss: 0.171227\n",
      "Epoch 8, train loss: 0.193752\n",
      "Epoch 9, train loss: 0.164758\n",
      "Epoch 10, train loss: 0.143362\n",
      "Epoch 11, train loss: 0.185258\n",
      "Epoch 12, train loss: 0.186673\n",
      "Epoch 13, train loss: 0.149610\n",
      "Epoch 14, train loss: 0.169796\n",
      "Epoch 15, train loss: 0.161940\n",
      "Epoch 16, train loss: 0.229369\n",
      "Epoch 17, train loss: 0.206049\n",
      "Epoch 18, train loss: 0.177832\n",
      "Epoch 19, train loss: 0.178387\n",
      "2020-11-24 21:08:33\n",
      "Reading 2 segments\n",
      "Building dataset, requesting data from 0 to 2\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 2 continuous time series\n",
      "Data shape: (1463, 7), Train/test: 1/1462\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2020-11-02 00:02:56\n",
      "Reading 40 segments\n",
      "Building dataset, requesting data from 0 to 40\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 403/5198\n",
      "Found 40 continuous time series\n",
      "Data shape: (5603, 7), Train/test: 5601/2\n",
      "Train test ratio: 2800.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_t1dexi.CGMSDataSeg object at 0x0000021FCCF06500>\n",
      "Epoch 0, test loss: 0.194380\n",
      "Epoch 1, test loss: 0.187417\n",
      "Epoch 2, test loss: 0.185976\n",
      "Epoch 3, test loss: 0.188830\n",
      "Epoch 4, test loss: 0.191134\n",
      "Epoch 5, test loss: 0.184650\n",
      "Epoch 6, test loss: 0.184545\n",
      "Epoch 7, test loss: 0.183212\n",
      "Epoch 8, test loss: 0.183169\n",
      "Epoch 9, test loss: 0.192505\n",
      "Epoch 10, test loss: 0.184437\n",
      "Epoch 11, test loss: 0.183455\n",
      "Epoch 12, test loss: 0.187557\n",
      "Epoch 13, test loss: 0.183804\n",
      "Epoch 14, test loss: 0.182204\n",
      "Epoch 15, test loss: 0.181850\n",
      "Epoch 16, test loss: 0.181591\n",
      "Epoch 17, test loss: 0.181357\n",
      "Epoch 18, test loss: 0.186753\n",
      "Epoch 19, test loss: 0.181875\n",
      "[[1.99       1.84370387 1.99       1.81589139]\n",
      " [1.99       1.87170255 1.99       1.8647778 ]\n",
      " [2.         1.90564144 2.         1.90297818]\n",
      " ...\n",
      " [0.96       1.0338186  0.96       1.09776008]\n",
      " [0.99       1.12270343 0.99       1.19240022]\n",
      " [1.03       1.10373127 1.03       1.13935149]]\n",
      "Pretrain data: 12530153.0\n",
      "Building dataset, requesting data from 0 to 292\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6406/80486\n",
      "Found 292 continuous time series\n",
      "Data shape: (86894, 7), Train/test: 86892/2\n",
      "Train test ratio: 43446.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 7), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 7), dtype=float32)\n",
      "line73: Shape of y: (None, 7)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1357\n",
      "Epoch 0, train loss: 0.149894\n",
      "Epoch 1, train loss: 0.209589\n",
      "Epoch 2, train loss: 0.191212\n",
      "Epoch 3, train loss: 0.190582\n",
      "Epoch 4, train loss: 0.146260\n",
      "Epoch 5, train loss: 0.257421\n",
      "Epoch 6, train loss: 0.178833\n",
      "Epoch 7, train loss: 0.223152\n",
      "Epoch 8, train loss: 0.181549\n",
      "Epoch 9, train loss: 0.147499\n",
      "Epoch 10, train loss: 0.178806\n",
      "Epoch 11, train loss: 0.162055\n",
      "Epoch 12, train loss: 0.176110\n",
      "Epoch 13, train loss: 0.195005\n",
      "Epoch 14, train loss: 0.204705\n",
      "Epoch 15, train loss: 0.226411\n",
      "Epoch 16, train loss: 0.171327\n",
      "Epoch 17, train loss: 0.129418\n",
      "Epoch 18, train loss: 0.237917\n",
      "Epoch 19, train loss: 0.205627\n",
      "2020-04-23 13:38:03\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (1453, 7), Train/test: 1/1452\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2020-04-01 00:01:51\n",
      "Reading 8 segments\n",
      "Building dataset, requesting data from 0 to 8\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 399/5504\n",
      "Found 8 continuous time series\n",
      "Data shape: (5905, 7), Train/test: 5903/2\n",
      "Train test ratio: 2951.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_t1dexi.CGMSDataSeg object at 0x0000022049C99A20>\n",
      "Epoch 0, test loss: 0.114825\n",
      "Epoch 1, test loss: 0.112664\n",
      "Epoch 2, test loss: 0.113657\n",
      "Epoch 3, test loss: 0.113093\n",
      "Epoch 4, test loss: 0.115316\n",
      "Epoch 5, test loss: 0.113973\n",
      "Epoch 6, test loss: 0.119650\n",
      "Epoch 7, test loss: 0.118545\n",
      "Epoch 8, test loss: 0.113698\n",
      "Epoch 9, test loss: 0.117755\n",
      "Epoch 10, test loss: 0.114693\n",
      "Epoch 11, test loss: 0.115486\n",
      "Epoch 12, test loss: 0.114040\n",
      "Epoch 13, test loss: 0.115503\n",
      "Epoch 14, test loss: 0.113672\n",
      "Epoch 15, test loss: 0.113565\n",
      "Epoch 16, test loss: 0.114130\n",
      "Epoch 17, test loss: 0.114181\n",
      "Epoch 18, test loss: 0.114817\n",
      "Epoch 19, test loss: 0.113634\n",
      "[[1.05       1.09360528 1.05       1.07557392]\n",
      " [1.07       1.11732996 1.07       1.09575856]\n",
      " [1.08       1.0786221  1.08       1.06339192]\n",
      " ...\n",
      " [0.73       0.82045907 0.73       0.81688845]\n",
      " [0.74       0.80240721 0.74       0.80274451]\n",
      " [0.74       0.80539542 0.74       0.80438221]]\n",
      "Pretrain data: 12135935.0\n",
      "Building dataset, requesting data from 0 to 295\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6607/79884\n",
      "Found 295 continuous time series\n",
      "Data shape: (86493, 7), Train/test: 86491/2\n",
      "Train test ratio: 43245.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 7), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 7), dtype=float32)\n",
      "line73: Shape of y: (None, 7)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1351\n",
      "Epoch 0, train loss: 0.240513\n",
      "Epoch 1, train loss: 0.181335\n",
      "Epoch 2, train loss: 0.205161\n",
      "Epoch 3, train loss: 0.205697\n",
      "Epoch 4, train loss: 0.179614\n",
      "Epoch 5, train loss: 0.172818\n",
      "Epoch 6, train loss: 0.185918\n",
      "Epoch 7, train loss: 0.199624\n",
      "Epoch 8, train loss: 0.135026\n",
      "Epoch 9, train loss: 0.181772\n",
      "Epoch 10, train loss: 0.160231\n",
      "Epoch 11, train loss: 0.172970\n",
      "Epoch 12, train loss: 0.172596\n",
      "Epoch 13, train loss: 0.165921\n",
      "Epoch 14, train loss: 0.177356\n",
      "Epoch 15, train loss: 0.184565\n",
      "Epoch 16, train loss: 0.136785\n",
      "Epoch 17, train loss: 0.150779\n",
      "Epoch 18, train loss: 0.115425\n",
      "Epoch 19, train loss: 0.152727\n",
      "2021-01-07 11:34:54\n",
      "Reading 2 segments\n",
      "Building dataset, requesting data from 0 to 2\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 2 continuous time series\n",
      "Data shape: (1577, 7), Train/test: 1/1576\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2020-12-16 00:41:37\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 198/6106\n",
      "Found 5 continuous time series\n",
      "Data shape: (6306, 7), Train/test: 6304/2\n",
      "Train test ratio: 3152.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_t1dexi.CGMSDataSeg object at 0x00000220454FBD30>\n",
      "Epoch 0, test loss: 0.198904\n",
      "Epoch 1, test loss: 0.191807\n",
      "Epoch 2, test loss: 0.190447\n",
      "Epoch 3, test loss: 0.190692\n",
      "Epoch 4, test loss: 0.190072\n",
      "Epoch 5, test loss: 0.190264\n",
      "Epoch 6, test loss: 0.189728\n",
      "Epoch 7, test loss: 0.192145\n",
      "Epoch 8, test loss: 0.188798\n",
      "Epoch 9, test loss: 0.191318\n",
      "Epoch 10, test loss: 0.196589\n",
      "Epoch 11, test loss: 0.189304\n",
      "Epoch 12, test loss: 0.190180\n",
      "Epoch 13, test loss: 0.189410\n",
      "Epoch 14, test loss: 0.192185\n",
      "Epoch 15, test loss: 0.189018\n",
      "Epoch 16, test loss: 0.191282\n",
      "Epoch 17, test loss: 0.188227\n",
      "Epoch 18, test loss: 0.188624\n",
      "Epoch 19, test loss: 0.188992\n",
      "[[1.09       1.01395965 1.09       1.05348408]\n",
      " [1.08       1.01980996 1.08       1.08546317]\n",
      " [1.06       1.00028825 1.06       1.06958842]\n",
      " ...\n",
      " [2.05       2.04180074 2.05       2.13919687]\n",
      " [2.02       2.04811168 2.02       2.14263082]\n",
      " [1.97       2.02268147 1.97       2.10917091]]\n",
      "Pretrain data: 12203462.0\n",
      "Building dataset, requesting data from 0 to 297\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6437/79988\n",
      "Found 297 continuous time series\n",
      "Data shape: (86427, 7), Train/test: 86425/2\n",
      "Train test ratio: 43212.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 7), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 7), dtype=float32)\n",
      "line73: Shape of y: (None, 7)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1350\n",
      "Epoch 0, train loss: 0.193061\n",
      "Epoch 1, train loss: 0.167164\n",
      "Epoch 2, train loss: 0.186082\n",
      "Epoch 3, train loss: 0.196966\n",
      "Epoch 4, train loss: 0.147111\n",
      "Epoch 5, train loss: 0.189845\n",
      "Epoch 6, train loss: 0.163574\n",
      "Epoch 7, train loss: 0.216300\n",
      "Epoch 8, train loss: 0.208367\n",
      "Epoch 9, train loss: 0.209523\n",
      "Epoch 10, train loss: 0.134306\n",
      "Epoch 11, train loss: 0.186757\n",
      "Epoch 12, train loss: 0.167478\n",
      "Epoch 13, train loss: 0.184881\n",
      "Epoch 14, train loss: 0.173124\n",
      "Epoch 15, train loss: 0.147625\n",
      "Epoch 16, train loss: 0.164602\n",
      "Epoch 17, train loss: 0.191887\n",
      "Epoch 18, train loss: 0.154072\n",
      "Epoch 19, train loss: 0.130907\n",
      "2021-01-19 10:32:00\n",
      "Reading 2 segments\n",
      "Building dataset, requesting data from 0 to 2\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 2 continuous time series\n",
      "Data shape: (1590, 7), Train/test: 1/1589\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2020-12-28 00:00:56\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 368/6002\n",
      "Found 3 continuous time series\n",
      "Data shape: (6372, 7), Train/test: 6370/2\n",
      "Train test ratio: 3185.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_t1dexi.CGMSDataSeg object at 0x0000022049C09930>\n",
      "Epoch 0, test loss: 0.175964\n",
      "Epoch 1, test loss: 0.165314\n",
      "Epoch 2, test loss: 0.164206\n",
      "Epoch 3, test loss: 0.172445\n",
      "Epoch 4, test loss: 0.163319\n",
      "Epoch 5, test loss: 0.163410\n",
      "Epoch 6, test loss: 0.161718\n",
      "Epoch 7, test loss: 0.169415\n",
      "Epoch 8, test loss: 0.167823\n",
      "Epoch 9, test loss: 0.159555\n",
      "Epoch 10, test loss: 0.165386\n",
      "Epoch 11, test loss: 0.164282\n",
      "Epoch 12, test loss: 0.162213\n",
      "Epoch 13, test loss: 0.161155\n",
      "Epoch 14, test loss: 0.165829\n",
      "Epoch 15, test loss: 0.164510\n",
      "Epoch 16, test loss: 0.159164\n",
      "Epoch 17, test loss: 0.160961\n",
      "Epoch 18, test loss: 0.162981\n",
      "Epoch 19, test loss: 0.159945\n",
      "[[2.23       2.02122116 2.23       2.0024662 ]\n",
      " [2.19       2.15700388 2.19       2.07410765]\n",
      " [2.11       2.32842207 2.11       2.18011832]\n",
      " ...\n",
      " [1.02       0.83580041 1.02       0.84102154]\n",
      " [1.05       0.85461891 1.05       0.88789952]\n",
      " [1.01       0.90702873 1.01       0.95130157]]\n",
      "Pretrain data: 12249115.0\n",
      "Building dataset, requesting data from 0 to 288\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6594/80053\n",
      "Found 288 continuous time series\n",
      "Data shape: (86649, 7), Train/test: 86647/2\n",
      "Train test ratio: 43323.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 7), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 7), dtype=float32)\n",
      "line73: Shape of y: (None, 7)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1353\n",
      "Epoch 0, train loss: 0.228771\n",
      "Epoch 1, train loss: 0.257458\n",
      "Epoch 2, train loss: 0.207016\n",
      "Epoch 3, train loss: 0.164449\n",
      "Epoch 4, train loss: 0.206301\n",
      "Epoch 5, train loss: 0.132318\n",
      "Epoch 6, train loss: 0.147690\n",
      "Epoch 7, train loss: 0.153654\n",
      "Epoch 8, train loss: 0.188188\n",
      "Epoch 9, train loss: 0.148173\n",
      "Epoch 10, train loss: 0.152651\n",
      "Epoch 11, train loss: 0.169208\n",
      "Epoch 12, train loss: 0.166160\n",
      "Epoch 13, train loss: 0.206354\n",
      "Epoch 14, train loss: 0.154478\n",
      "Epoch 15, train loss: 0.151653\n",
      "Epoch 16, train loss: 0.196846\n",
      "Epoch 17, train loss: 0.174595\n",
      "Epoch 18, train loss: 0.192817\n",
      "Epoch 19, train loss: 0.185687\n",
      "2020-11-20 12:58:07\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 3 continuous time series\n",
      "Data shape: (1547, 7), Train/test: 1/1546\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2020-10-29 00:03:17\n",
      "Reading 12 segments\n",
      "Building dataset, requesting data from 0 to 12\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 211/5937\n",
      "Found 12 continuous time series\n",
      "Data shape: (6150, 7), Train/test: 6148/2\n",
      "Train test ratio: 3074.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_t1dexi.CGMSDataSeg object at 0x0000021FCC9D95D0>\n",
      "Epoch 0, test loss: 0.215810\n",
      "Epoch 1, test loss: 0.217841\n",
      "Epoch 2, test loss: 0.222201\n",
      "Epoch 3, test loss: 0.225487\n",
      "Epoch 4, test loss: 0.220564\n",
      "Epoch 5, test loss: 0.216690\n",
      "Epoch 6, test loss: 0.220866\n",
      "Epoch 7, test loss: 0.231071\n",
      "Epoch 8, test loss: 0.213930\n",
      "Epoch 9, test loss: 0.216262\n",
      "Epoch 10, test loss: 0.217499\n",
      "Epoch 11, test loss: 0.217240\n",
      "Epoch 12, test loss: 0.222199\n",
      "Epoch 13, test loss: 0.215751\n",
      "Epoch 14, test loss: 0.218107\n",
      "Epoch 15, test loss: 0.216137\n",
      "Epoch 16, test loss: 0.222637\n",
      "Epoch 17, test loss: 0.218997\n",
      "Epoch 18, test loss: 0.215683\n",
      "Epoch 19, test loss: 0.218458\n",
      "[[1.06       1.22137105 1.06       1.26164031]\n",
      " [1.05       1.11006963 1.05       1.18417156]\n",
      " [1.06       1.00448775 1.06       1.09784901]\n",
      " ...\n",
      " [1.12       1.2197907  1.12       1.25850248]\n",
      " [1.1        1.17291451 1.1        1.22716165]\n",
      " [1.09       1.0834626  1.09       1.16305745]]\n",
      "Pretrain data: 12460029.0\n",
      "Building dataset, requesting data from 0 to 127\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6307/82072\n",
      "Found 127 continuous time series\n",
      "Data shape: (88381, 7), Train/test: 88379/2\n",
      "Train test ratio: 44189.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 7), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 7), dtype=float32)\n",
      "line73: Shape of y: (None, 7)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1380\n",
      "Epoch 0, train loss: 0.200206\n",
      "Epoch 1, train loss: 0.224433\n",
      "Epoch 2, train loss: 0.196028\n",
      "Epoch 3, train loss: 0.181295\n",
      "Epoch 4, train loss: 0.143644\n",
      "Epoch 5, train loss: 0.165092\n",
      "Epoch 6, train loss: 0.169190\n",
      "Epoch 7, train loss: 0.150374\n",
      "Epoch 8, train loss: 0.201742\n",
      "Epoch 9, train loss: 0.177624\n",
      "Epoch 10, train loss: 0.192907\n",
      "Epoch 11, train loss: 0.199143\n",
      "Epoch 12, train loss: 0.221642\n",
      "Epoch 13, train loss: 0.175999\n",
      "Epoch 14, train loss: 0.142752\n",
      "Epoch 15, train loss: 0.176337\n",
      "Epoch 16, train loss: 0.173686\n",
      "Epoch 17, train loss: 0.151009\n",
      "Epoch 18, train loss: 0.147806\n",
      "Epoch 19, train loss: 0.169905\n",
      "2020-03-31 09:39:25\n",
      "Reading 38 segments\n",
      "Building dataset, requesting data from 0 to 38\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 38 continuous time series\n",
      "Data shape: (1134, 7), Train/test: 1/1133\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2020-03-09 00:03:08\n",
      "Reading 173 segments\n",
      "Building dataset, requesting data from 0 to 173\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 498/3918\n",
      "Found 173 continuous time series\n",
      "Data shape: (4418, 7), Train/test: 4416/2\n",
      "Train test ratio: 2208.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_t1dexi.CGMSDataSeg object at 0x0000021FCCF6C6A0>\n",
      "Epoch 0, test loss: 0.143599\n",
      "Epoch 1, test loss: 0.142498\n",
      "Epoch 2, test loss: 0.144880\n",
      "Epoch 3, test loss: 0.143857\n",
      "Epoch 4, test loss: 0.142431\n",
      "Epoch 5, test loss: 0.144382\n",
      "Epoch 6, test loss: 0.143791\n",
      "Epoch 7, test loss: 0.142663\n",
      "Epoch 8, test loss: 0.143549\n",
      "Epoch 9, test loss: 0.142459\n",
      "Epoch 10, test loss: 0.142475\n",
      "Epoch 11, test loss: 0.142614\n",
      "Epoch 12, test loss: 0.145434\n",
      "Epoch 13, test loss: 0.143538\n",
      "Epoch 14, test loss: 0.144527\n",
      "Epoch 15, test loss: 0.143397\n",
      "Epoch 16, test loss: 0.142074\n",
      "Epoch 17, test loss: 0.141281\n",
      "Epoch 18, test loss: 0.143151\n",
      "Epoch 19, test loss: 0.141990\n",
      "[[0.92       0.98271716 0.92       0.96946543]\n",
      " [0.9        0.99394989 0.9        0.98063064]\n",
      " [0.89       0.98520792 0.89       0.9730351 ]\n",
      " ...\n",
      " [0.95       0.98960984 0.95       0.98198622]\n",
      " [0.95       0.98216873 0.95       0.97717887]\n",
      " [0.96       0.96998221 0.96       0.9615193 ]]\n",
      "Pretrain data: 12071154.0\n",
      "Building dataset, requesting data from 0 to 297\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6558/79906\n",
      "Found 297 continuous time series\n",
      "Data shape: (86466, 7), Train/test: 86464/2\n",
      "Train test ratio: 43232.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 7), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 7), dtype=float32)\n",
      "line73: Shape of y: (None, 7)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1351\n",
      "Epoch 0, train loss: 0.182111\n",
      "Epoch 1, train loss: 0.176752\n",
      "Epoch 2, train loss: 0.171551\n",
      "Epoch 3, train loss: 0.214448\n",
      "Epoch 4, train loss: 0.168493\n",
      "Epoch 5, train loss: 0.159995\n",
      "Epoch 6, train loss: 0.141112\n",
      "Epoch 7, train loss: 0.187214\n",
      "Epoch 8, train loss: 0.214881\n",
      "Epoch 9, train loss: 0.141677\n",
      "Epoch 10, train loss: 0.151902\n",
      "Epoch 11, train loss: 0.136593\n",
      "Epoch 12, train loss: 0.182708\n",
      "Epoch 13, train loss: 0.243467\n",
      "Epoch 14, train loss: 0.180215\n",
      "Epoch 15, train loss: 0.157657\n",
      "Epoch 16, train loss: 0.197638\n",
      "Epoch 17, train loss: 0.196970\n",
      "Epoch 18, train loss: 0.113727\n",
      "Epoch 19, train loss: 0.189703\n",
      "2020-10-02 08:04:09\n",
      "Reading 4 segments\n",
      "Building dataset, requesting data from 0 to 4\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 4 continuous time series\n",
      "Data shape: (1557, 7), Train/test: 1/1556\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2020-09-10 00:02:31\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 247/6084\n",
      "Found 3 continuous time series\n",
      "Data shape: (6333, 7), Train/test: 6331/2\n",
      "Train test ratio: 3165.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_t1dexi.CGMSDataSeg object at 0x0000022044FF7670>\n",
      "Epoch 0, test loss: 0.205588\n",
      "Epoch 1, test loss: 0.199739\n",
      "Epoch 2, test loss: 0.198727\n",
      "Epoch 3, test loss: 0.199334\n",
      "Epoch 4, test loss: 0.198559\n",
      "Epoch 5, test loss: 0.198025\n",
      "Epoch 6, test loss: 0.197465\n",
      "Epoch 7, test loss: 0.204735\n",
      "Epoch 8, test loss: 0.196390\n",
      "Epoch 9, test loss: 0.197226\n",
      "Epoch 10, test loss: 0.201184\n",
      "Epoch 11, test loss: 0.200553\n",
      "Epoch 12, test loss: 0.203415\n",
      "Epoch 13, test loss: 0.196342\n",
      "Epoch 14, test loss: 0.195647\n",
      "Epoch 15, test loss: 0.196985\n",
      "Epoch 16, test loss: 0.200169\n",
      "Epoch 17, test loss: 0.200371\n",
      "Epoch 18, test loss: 0.194895\n",
      "Epoch 19, test loss: 0.194691\n",
      "[[1.16       1.18709612 1.16       1.27978253]\n",
      " [1.15       1.16249847 1.15       1.26146519]\n",
      " [1.14       1.13722825 1.14       1.23752558]\n",
      " ...\n",
      " [1.92       1.67588711 1.92       1.78560674]\n",
      " [1.92       1.72732043 1.92       1.85610008]\n",
      " [1.93       1.76106644 1.93       1.87265801]]\n",
      "Pretrain data: 12519714.0\n",
      "Building dataset, requesting data from 0 to 297\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 5692/80770\n",
      "Found 297 continuous time series\n",
      "Data shape: (86464, 7), Train/test: 86462/2\n",
      "Train test ratio: 43231.00\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 7), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 7), dtype=float32)\n",
      "line73: Shape of y: (None, 7)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1350\n",
      "Epoch 0, train loss: 0.197710\n",
      "Epoch 1, train loss: 0.176984\n",
      "Epoch 2, train loss: 0.175675\n",
      "Epoch 3, train loss: 0.166683\n",
      "Epoch 4, train loss: 0.178943\n",
      "Epoch 5, train loss: 0.170011\n",
      "Epoch 6, train loss: 0.146982\n",
      "Epoch 7, train loss: 0.186915\n",
      "Epoch 8, train loss: 0.210357\n",
      "Epoch 9, train loss: 0.222916\n",
      "Epoch 10, train loss: 0.133466\n",
      "Epoch 11, train loss: 0.213949\n",
      "Epoch 12, train loss: 0.174305\n",
      "Epoch 13, train loss: 0.156096\n",
      "Epoch 14, train loss: 0.175904\n",
      "Epoch 15, train loss: 0.171507\n",
      "Epoch 16, train loss: 0.159241\n",
      "Epoch 17, train loss: 0.208565\n",
      "Epoch 18, train loss: 0.216597\n",
      "Epoch 19, train loss: 0.187457\n",
      "2019-12-27 07:57:15\n",
      "Reading 4 segments\n",
      "Building dataset, requesting data from 0 to 4\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 4 continuous time series\n",
      "Data shape: (1557, 7), Train/test: 1/1556\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2019-12-05 00:02:15\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 1113/5220\n",
      "Found 3 continuous time series\n",
      "Data shape: (6335, 7), Train/test: 6333/2\n",
      "Train test ratio: 3166.50\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_t1dexi.CGMSDataSeg object at 0x0000022049C4D7B0>\n",
      "Epoch 0, test loss: 0.125946\n",
      "Epoch 1, test loss: 0.126580\n",
      "Epoch 2, test loss: 0.125414\n",
      "Epoch 3, test loss: 0.124291\n",
      "Epoch 4, test loss: 0.125114\n",
      "Epoch 5, test loss: 0.124291\n",
      "Epoch 6, test loss: 0.124181\n",
      "Epoch 7, test loss: 0.124707\n",
      "Epoch 8, test loss: 0.123844\n",
      "Epoch 9, test loss: 0.124152\n",
      "Epoch 10, test loss: 0.127732\n",
      "Epoch 11, test loss: 0.123687\n",
      "Epoch 12, test loss: 0.124192\n",
      "Epoch 13, test loss: 0.123661\n",
      "Epoch 14, test loss: 0.124480\n",
      "Epoch 15, test loss: 0.124133\n",
      "Epoch 16, test loss: 0.126750\n",
      "Epoch 17, test loss: 0.123688\n",
      "Epoch 18, test loss: 0.123267\n",
      "Epoch 19, test loss: 0.127257\n",
      "[[1.25       1.59076631 1.25       1.51347566]\n",
      " [1.18       1.52809477 1.18       1.47864985]\n",
      " [1.13       1.40927577 1.13       1.35288978]\n",
      " ...\n",
      " [0.8        0.90233296 0.8        0.84614265]\n",
      " [0.8        0.90753418 0.8        0.87172079]\n",
      " [0.79       0.9031443  0.79       0.86616284]]\n",
      "Pretrain data: 12312251.0\n",
      "Building dataset, requesting data from 0 to 297\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 6279/80180\n",
      "Found 297 continuous time series\n",
      "Data shape: (86461, 7), Train/test: 86459/2\n",
      "Train test ratio: 43229.50\n",
      "################################################################################\n",
      "Feature size is: \n",
      "0\n",
      "In regressor, x =\n",
      "Tensor(\"x:0\", shape=(None, 7), dtype=float32)\n",
      "In regressor, y =\n",
      "Tensor(\"add:0\", shape=(None, 7), dtype=float32)\n",
      "line73: Shape of y: (None, 7)\n",
      "Before L2 regularization\n",
      "Before training for loop\n",
      "int(low_fid_data.train_n / batch_size) =  1350\n",
      "Epoch 0, train loss: 0.178132\n",
      "Epoch 1, train loss: 0.182510\n",
      "Epoch 2, train loss: 0.125219\n",
      "Epoch 3, train loss: 0.176922\n",
      "Epoch 4, train loss: 0.226289\n",
      "Epoch 5, train loss: 0.137463\n",
      "Epoch 6, train loss: 0.225952\n",
      "Epoch 7, train loss: 0.124412\n",
      "Epoch 8, train loss: 0.178639\n",
      "Epoch 9, train loss: 0.166509\n",
      "Epoch 10, train loss: 0.194447\n",
      "Epoch 11, train loss: 0.202748\n",
      "Epoch 12, train loss: 0.159203\n",
      "Epoch 13, train loss: 0.182392\n",
      "Epoch 14, train loss: 0.221060\n",
      "Epoch 15, train loss: 0.160187\n",
      "Epoch 16, train loss: 0.149862\n",
      "Epoch 17, train loss: 0.196924\n",
      "Epoch 18, train loss: 0.193492\n",
      "Epoch 19, train loss: 0.158608\n",
      "2021-01-12 07:22:48\n",
      "Reading 5 segments\n",
      "Building dataset, requesting data from 0 to 5\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 0/1\n",
      "Found 5 continuous time series\n",
      "Data shape: (1546, 7), Train/test: 1/1545\n",
      "Train test ratio: 0.00\n",
      "################################################################################\n",
      "2020-12-21 00:01:48\n",
      "Reading 3 segments\n",
      "Building dataset, requesting data from 0 to 3\n",
      "Train data requested beyond limit, using all but last one\n",
      "############################ Data structure summary ############################\n",
      "Hypo/no_hypo: 526/5810\n",
      "Found 3 continuous time series\n",
      "Data shape: (6338, 7), Train/test: 6336/2\n",
      "Train test ratio: 3168.00\n",
      "################################################################################\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "------------------in transfer----------------------\n",
      "INFO:tensorflow:Restoring parameters from ../t1dexi_results\\ph_6_rmse\\pretrain\n",
      "train_dataset\n",
      "<cgms_data_seg_t1dexi.CGMSDataSeg object at 0x0000022044E59FF0>\n",
      "Epoch 0, test loss: 0.167244\n",
      "Epoch 1, test loss: 0.167666\n",
      "Epoch 2, test loss: 0.165396\n",
      "Epoch 3, test loss: 0.169829\n",
      "Epoch 4, test loss: 0.169023\n",
      "Epoch 5, test loss: 0.166184\n",
      "Epoch 6, test loss: 0.165927\n",
      "Epoch 7, test loss: 0.164689\n",
      "Epoch 8, test loss: 0.164727\n",
      "Epoch 9, test loss: 0.164821\n",
      "Epoch 10, test loss: 0.164457\n",
      "Epoch 11, test loss: 0.165298\n",
      "Epoch 12, test loss: 0.164743\n",
      "Epoch 13, test loss: 0.165677\n",
      "Epoch 14, test loss: 0.179694\n",
      "Epoch 15, test loss: 0.164500\n",
      "Epoch 16, test loss: 0.166222\n",
      "Epoch 17, test loss: 0.165938\n",
      "Epoch 18, test loss: 0.164631\n",
      "Epoch 19, test loss: 0.167000\n",
      "[[1.73       1.66137242 1.73       1.61187887]\n",
      " [1.73       1.66497731 1.73       1.62634838]\n",
      " [1.74       1.60052586 1.74       1.54202628]\n",
      " ...\n",
      " [2.91       3.16189265 2.91       3.15222716]\n",
      " [2.86       2.96670914 2.86       2.89578867]\n",
      " [2.82       2.90561652 2.82       2.89843369]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Mismatch between array dtype ('<U32') and format specifier ('%s %.4f %.4f')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\baiyi\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\numpy\\lib\\npyio.py:1623\u001b[0m, in \u001b[0;36msavetxt\u001b[1;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1623\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m newline\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not numpy.str_",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m     all_errs\u001b[38;5;241m.\u001b[39mappend([pid] \u001b[38;5;241m+\u001b[39m errs)\n\u001b[0;32m     86\u001b[0m all_errs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_errs)\n\u001b[1;32m---> 87\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavetxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/errors.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_errs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m%.4f\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m%.4f\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# %.4f %.4f\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# label pair:(groundTruth, y_pred)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\baiyi\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\numpy\\lib\\npyio.py:1625\u001b[0m, in \u001b[0;36msavetxt\u001b[1;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[0;32m   1623\u001b[0m             v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtuple\u001b[39m(row) \u001b[38;5;241m+\u001b[39m newline\n\u001b[0;32m   1624\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1625\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch between array dtype (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1626\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat specifier (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1627\u001b[0m                             \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mstr\u001b[39m(X\u001b[38;5;241m.\u001b[39mdtype), \u001b[38;5;28mformat\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m         fh\u001b[38;5;241m.\u001b[39mwrite(v)\n\u001b[0;32m   1630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(footer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Mismatch between array dtype ('<U32') and format specifier ('%s %.4f %.4f')"
     ]
    }
   ],
   "source": [
    "# Loop\n",
    "standard = False  # do not use standard\n",
    "all_errs = []\n",
    "for pid in overlap:\n",
    "    # local_train_data = []\n",
    "    \n",
    "    # for k in train_data.keys():\n",
    "    #     if k != pid:\n",
    "    #         local_train_data += train_data[k]\n",
    "    train_pids = set(overlap) - set([pid])\n",
    "    local_train_data = []\n",
    "    for k in train_pids:\n",
    "        local_train_data += train_data[k + \"_training_data\"]\n",
    "    print(f\"Pretrain data: {sum([sum(x) for x in local_train_data])}\")\n",
    "\n",
    "    train_dataset.data = local_train_data\n",
    "    train_dataset.set_cutpoint = -1\n",
    "    train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    regressor(train_dataset, *argv, l_type, outdir)\n",
    "    # Fine-tune and test\n",
    "    # target_test_dataset = CGMSDataSeg(\n",
    "    #     \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\", 5\n",
    "    # )\n",
    "    target_test_dataset = CGMSDataSeg(\n",
    "    \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/BGprediction/LB_split/testing/{pid}_testing_data.csv\", 5\n",
    "    )\n",
    "    target_test_dataset.set_cutpoint = 1\n",
    "    target_test_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        0.01,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    # target_train_dataset = CGMSDataSeg(\n",
    "    #     \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/BGprediction/OhioT1DM/2018/test/{pid}-ws-testing.xml\", 5\n",
    "    # )\n",
    "    target_train_dataset = CGMSDataSeg(\n",
    "    \"ohio\", f\"C:/Users/baiyi/OneDrive/Desktop/BGprediction/LB_split/training/{pid}_training_data.csv\", 5\n",
    "    )\n",
    "    target_train_dataset.set_cutpoint = -1\n",
    "    target_train_dataset.reset(\n",
    "        sampling_horizon,\n",
    "        prediction_horizon,\n",
    "        scale,\n",
    "        100,\n",
    "        False,\n",
    "        outtype,\n",
    "        1,\n",
    "        standard,\n",
    "    )\n",
    "    err, labels = test_ckpt(target_test_dataset, outdir)\n",
    "    errs = [err]\n",
    "    transfer_res = [labels]\n",
    "    for i in range(1, 2):\n",
    "        err, labels = regressor_transfer(\n",
    "            target_train_dataset,\n",
    "            target_test_dataset,\n",
    "            config[\"batch_size\"],\n",
    "            epoch,\n",
    "            outdir,\n",
    "            i,\n",
    "        )\n",
    "        errs.append(err)\n",
    "        transfer_res.append(labels)\n",
    "    transfer_res = np.concatenate(transfer_res, axis=1)\n",
    "    print(transfer_res)\n",
    "    np.savetxt(\n",
    "        f\"{outdir}/{pid}.txt\",\n",
    "        transfer_res,\n",
    "        fmt=\"%.4f %.4f %.4f %.4f\", #%.4f %.4f %.4f %.4f\n",
    "    )\n",
    "    all_errs.append([pid] + errs)\n",
    "all_errs = np.array(all_errs)\n",
    "np.savetxt(f\"{outdir}/errors.txt\", all_errs, fmt=\"%s %.4f %.4f\") # %.4f %.4f\n",
    "# label pair:(groundTruth, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['854', '0.15074661', '0.14032947'],\n",
    "['979', '0.10964842', '0.1068475'],\n",
    "['816', '0.17296007', '0.15637168'],\n",
    "['953', '0.16875254', '0.16648282'],\n",
    "['981', '0.18782552', '0.1781756'],\n",
    "['1617', '0.18750052', '0.17405139'],\n",
    "['1343', '0.1479887', '0.12966709'],\n",
    "['987', '0.1956755', '0.18187545'],\n",
    "['255', '0.112035595', '0.11363354'],\n",
    "['907', '0.2046883', '0.18899176'],\n",
    "['856', '0.16526204', '0.15994464'],\n",
    "['354', '0.21569382', '0.21845779'],\n",
    "['894', '0.1430959', '0.14199'],\n",
    "['862', '0.2226933', '0.19469088'],\n",
    "['900', '0.14939056', '0.12725726'],\n",
    "['695', '0.16643564', '0.16699977']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['854', '0.15074661', '0.14032947'],\n",
       "       ['979', '0.10964842', '0.1068475'],\n",
       "       ['816', '0.17296007', '0.15637168'],\n",
       "       ['953', '0.16875254', '0.16648282'],\n",
       "       ['981', '0.18782552', '0.1781756'],\n",
       "       ['1617', '0.18750052', '0.17405139'],\n",
       "       ['1343', '0.1479887', '0.12966709'],\n",
       "       ['987', '0.1956755', '0.18187545'],\n",
       "       ['255', '0.112035595', '0.11363354'],\n",
       "       ['907', '0.2046883', '0.18899176'],\n",
       "       ['856', '0.16526204', '0.15994464'],\n",
       "       ['354', '0.21569382', '0.21845779'],\n",
       "       ['894', '0.1430959', '0.14199'],\n",
       "       ['862', '0.2226933', '0.19469088'],\n",
       "       ['900', '0.14939056', '0.12725726'],\n",
       "       ['695', '0.16643564', '0.16699977']], dtype='<U32')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the second column: 0.16877456468750002\n",
      "Average of the third column: 0.159110415\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert the second and third columns to floats\n",
    "second_column = all_errs[:, 1].astype(float)\n",
    "third_column = all_errs[:, 2].astype(float)\n",
    "\n",
    "# Calculate the average\n",
    "average_second_column = np.mean(second_column)\n",
    "average_third_column = np.mean(third_column)\n",
    "\n",
    "print(\"Average of the second column:\", average_second_column)\n",
    "print(\"Average of the third column:\", average_third_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['854', '0.14176467', '0.13840644'],\n",
    "['979', '0.11082648', '0.10547459'],\n",
    "['816', '0.17233326', '0.15507208'],\n",
    "['953', '0.16803236', '0.15838838'],\n",
    "['981', '0.18506993', '0.17628354'],\n",
    "['1617', '0.1810529', '0.17188448'],\n",
    "['1343', '0.14670542', '0.12550768'],\n",
    "['987', '0.1974195', '0.18113053'],\n",
    "['255', '0.11360771', '0.113014214'],\n",
    "['85', '0.24868742', '0.23800832'],\n",
    "['907', '0.198933', '0.1871577'],\n",
    "['856', '0.1641185', '0.16184378'],\n",
    "['354', '0.21777242', '0.22120297'],\n",
    "['894', '0.14986292', '0.1430042'],\n",
    "['911', '0.3157179', '0.30788115'],\n",
    "['862', '0.20412911', '0.19469687'],\n",
    "['900', '0.14108652', '0.1235286'],\n",
    "['695', '0.18116617', '0.16922566'] ## 30MIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_errs = np.array([['854', '0.14176467', '0.13840644'],\n",
    "['979', '0.11082648', '0.10547459'],\n",
    "['816', '0.17233326', '0.15507208'],\n",
    "['953', '0.16803236', '0.15838838'],\n",
    "['981', '0.18506993', '0.17628354'],\n",
    "['1617', '0.1810529', '0.17188448'],\n",
    "['1343', '0.14670542', '0.12550768'],\n",
    "['987', '0.1974195', '0.18113053'],\n",
    "['255', '0.11360771', '0.113014214'],\n",
    "['907', '0.198933', '0.1871577'],\n",
    "['856', '0.1641185', '0.16184378'],\n",
    "['354', '0.21777242', '0.22120297'],\n",
    "['894', '0.14986292', '0.1430042'],\n",
    "['862', '0.20412911', '0.19469687'],\n",
    "['900', '0.14108652', '0.1235286'],\n",
    "['695', '0.18116617', '0.16922566']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the second column: 0.16711755437499998\n",
      "Average of the third column: 0.157863857125\n"
     ]
    }
   ],
   "source": [
    "# Convert the second and third columns to floats\n",
    "second_column = all_errs[:, 1].astype(float)\n",
    "third_column = all_errs[:, 2].astype(float)\n",
    "\n",
    "# Calculate the average\n",
    "average_second_column = np.mean(second_column)\n",
    "average_third_column = np.mean(third_column)\n",
    "\n",
    "print(\"Average of the second column:\", average_second_column)\n",
    "print(\"Average of the third column:\", average_third_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The altimate result please go into the result folder to check the evaluation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If run into an error, but all result txt files are ready, please run this section for evaluation\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# List all files and directories in the current directory\n",
    "files_and_directories = os.listdir('./t1dexi_results')\n",
    "\n",
    "# Filter for files that end with .txt\n",
    "txt_files = [file for file in files_and_directories if file.endswith('.txt')]\n",
    "\n",
    "# Read the data from the text file\n",
    "def calcuate_rmse(file):\n",
    "    data = np.loadtxt(file)  # Make sure to replace 'data.txt' with your actual file path\n",
    "    print(file)\n",
    "    # Splitting the data into groundtruth and predictions\n",
    "    groundtruth = data[:, 0]  # First column as ground truth (also same as third column)\n",
    "    predictions_1 = data[:, 1]  # Second column as predictions from method 1\n",
    "    predictions_2 = data[:, 3]  # Fourth column as predictions from method 2\n",
    "\n",
    "    # Function to calculate RMSE\n",
    "    def calculate_rmse(true_values, predictions):\n",
    "        mse = np.mean((true_values - predictions) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return rmse\n",
    "\n",
    "    # Calculate RMSE for each method\n",
    "    rmse_method_1 = calculate_rmse(groundtruth, predictions_1)\n",
    "    rmse_method_2 = calculate_rmse(groundtruth, predictions_2)\n",
    "\n",
    "    print(\"RMSE for Method 1:\", rmse_method_1)\n",
    "    print(\"RMSE for Method 2:\", rmse_method_2)\n",
    "    return rmse_method_1\n",
    "\n",
    "\n",
    "rmse_list = []\n",
    "for f in txt_files[:-1]:\n",
    "    rmse1 = calcuate_rmse(f)\n",
    "    print(rmse1)\n",
    "    rmse_list.append(rmse1)\n",
    "\n",
    "print(np.average(rmse_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
